{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stopwords(stopwords_path):\n",
    "    with open(stopwords_path, 'r', encoding='utf-8') as f:\n",
    "        return [line.strip() for line in f]\n",
    "    \n",
    "def preprocess_data(corpus_path, stopwords):\n",
    "    corpus = []\n",
    "    with open(corpus_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            corpus.append(' '.join([word for word in jieba.lcut(line.strip()) if word not in stopwords]))\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "2020-06-18 09:23:39,300 : DEBUG : Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/m3/4yh806w92fdgcn0bk16ql7nw0000gn/T/jieba.cache\n",
      "2020-06-18 09:23:40,021 : DEBUG : Dumping model to file cache /var/folders/m3/4yh806w92fdgcn0bk16ql7nw0000gn/T/jieba.cache\n",
      "Loading model cost 0.779 seconds.\n",
      "2020-06-18 09:23:40,080 : DEBUG : Loading model cost 0.779 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "2020-06-18 09:23:40,081 : DEBUG : Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "n = 1000\n",
    "stopwords_path = \"../data/stop_words.txt\"\n",
    "documents_path = \"../data/documents_first_\" + str(n) + \".txt\"\n",
    "stopwords = load_stopwords(stopwords_path)\n",
    "documents = preprocess_data(documents_path, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'董明珠 惊人 之语 炮轰 美的 怒斥 国产车 炮火 引向 大众 这下 本来 习惯于 看热闹 吃 瓜 群众 答应 踩 同行 骂 竞争对手 意见 敢动 老子 一亩 三分 不行 014 月 日 晚间 格力电器 董事长 兼 总裁 董明珠 接受 采访 时 赞同 黄奇帆 取消 住房 公积金 说 格力电器 3700 套 房子 员工 入住 未来 格力 员工 发一 套房 公积金 听听 话 饱汉不知饿汉饥 专程来 炫富 经济 下行 大众 钱包 吃紧 格力 本事 每名 员工 分 房子 本事 格力 分房 取消 全国 公积金 站长 想 问 一句 董 小姐 蠢 坏 确实 格力 优秀 年 营业 收入 1981.53 拥有 万名 员工 格力 万名 员工 发一 套房 站长 说 信 格力 真 员工 分房 中国 企业 众多 能发 房子 企业 凤毛麟角 特别 受 疫情 影响 众多 行业 暴击 企业 濒临 倒闭 活着 不错 每人 发 套房 格力 员工 公积金 取消 公积金 格力 确实 省下 一大笔钱 我国 亿人 贫富差距 取消 公积金 势必会 影响 人群 利益 也许 董明珠 换种 表述 特定 福利 企业 取消 缴纳 公积金 企业 节约 成本 用于 研发 创新 公积金 整体 实施 影响 我国 当初 建立 住房 公积金 制度 新加坡 学习 希望 强制性 缴纳 办法 集合 政府 企业 职工 三方 力量 解决 民众 购房 中国 最先 实行 公积金 政策 上海 全国 房地产 市场 发展 实行 公房 分配制度 家庭 人均 住房面积 七八 平方米 住 拥挤 居住 环境 急需 改善 公积金 强制 缴存 看似 个人收入 减少 长期 并非如此 民企 公积金 缴纳 比例 5% 12% 薪资 基数 缴纳 比例 6% 公司 6% 12% 公积金 存缴 数额 50006% 别看 元不多 长此以往 可不是 小数 缴纳 时间 公积金 买房 提取 大众 福利 特别 事业单位 公务员 群体 公积金 缴纳 金额 高 一般来说 公务员 月 公积金 扣除 比例 工资 12% 公积金 政策 国家 补贴 数额 公务员 一个月 公积金 工资 24% 民企 两倍 账面 工资 特别 高 公务员 群体 公积金 住 建部 人民银行 总行 统计 显示 年 全国 住房 公积金 缴存 总额 14549.46 上年 增长 12.29% 缴存 人数 机关 事业单位 工作人员 国企 职工 缴存 住房 公积金 比例 占 年 缴存 总额 60.16% 占 高 年 城镇 私 民 营 企业 城镇 企业 缴纳 住房 公积金 占 总额 19.5% 公积金 公务员 群体 至关重要 普通人 公积金 房价 高昂 公积金 居民 低息 房贷 唯一 渠道 全国 住房 公积金 年 年度报告 显示 年末 累计 发放 住房贷款 3334.82 万笔 85821.32 人员 置业 首 套房 贷款 年 公积金 利率 3.25% 二套 3.75% 远 商贷 首 套房 贷款 平均 利率 5.5% 假设 贷款 年 期限 公积金 贷款 商业贷款 节省 利息 约 笔 不小 资金 取消 公积金 将会 损害 大部分 利益 举个 例子 刘 缴纳 公积金 比例 12% 月 缴存 652 年 3.25% 利率 贷款 购买 一套 住房 每月 还款 可用 公积金 冲抵 979 每月 还款 多元 公积金 减轻 购房 压力 用处 很大 缴纳 住房 公积金 好处 少 缴纳 个人所得税 计算 个税 减去 住房 公积金 数额 不论是 单位 缴纳 缴纳 实惠 很大 公积金 提取 不断扩大 公积金 效率 提升 买房 装修 可用 租房 大病 提取 当今 疫情 部门 发出通知 新冠 肺炎 患者 提取 住房 公积金 用于 医疗 支出 买房 账户 里 公积金 退休 取出 生活 改善 说 公积金 用处 只能 租房 买房 公积金 表面 事 关乎 国家 大部 利益 开发商 公积金 制度 降低 买房 门槛 买房 人会 有利于 房企 发展 地方 政府 开发商 有钱 买 缴纳 土地 出让金 有利于 地方 财政 说 专家 说 取消 公积金 可取 稍微 动动脑 子 甄别 利弊 类 声音 就行 跑 偏 免责 声明 本文 腾讯 新闻 客户端 媒体 代表 腾讯网 观点 立场'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [[word for word in document.split()] for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "texts = [[token for token in text if frequency[token] > 2] for text in texts]\n",
    "# pprint(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-18 09:23:54,127 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-06-18 09:23:54,281 : INFO : built Dictionary(11105 unique tokens: ['15%', '18%', '46%', '一周', '世界']...) from 1005 documents (total 200743 corpus positions)\n",
      "2020-06-18 09:23:54,282 : INFO : saving Dictionary object under ../data/first_1000_doc.dict, separately None\n",
      "2020-06-18 09:23:54,290 : INFO : saved ../data/first_1000_doc.dict\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(11105 unique tokens: ['15%', '18%', '46%', '一周', '世界']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.save('../data/first_' + str(n) + '_doc.dict')\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-18 09:23:54,400 : INFO : storing corpus in Matrix Market format to ../data/first_1000_doc.mm\n",
      "2020-06-18 09:23:54,401 : INFO : saving sparse matrix to ../data/first_1000_doc.mm\n",
      "2020-06-18 09:23:54,402 : INFO : PROGRESS: saving document #0\n",
      "2020-06-18 09:23:54,511 : INFO : PROGRESS: saving document #1000\n",
      "2020-06-18 09:23:54,512 : INFO : saved 1005x11105 matrix, density=0.982% (109589/11160525)\n",
      "2020-06-18 09:23:54,514 : INFO : saving MmCorpus index to ../data/first_1000_doc.mm.index\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize('../data/first_' + str(n) + '_doc.mm', corpus)\n",
    "# pprint(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-18 09:23:54,519 : INFO : collecting document frequencies\n",
      "2020-06-18 09:23:54,520 : INFO : PROGRESS: processing document #0\n",
      "2020-06-18 09:23:54,537 : INFO : calculating IDF weights for 1005 documents and 11105 features (109589 matrix non-zeros)\n"
     ]
    }
   ],
   "source": [
    "from gensim import models, similarities\n",
    "tf_idf = models.TfidfModel(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1),\n",
      "  (1, 1),\n",
      "  (2, 1),\n",
      "  (3, 1),\n",
      "  (4, 1),\n",
      "  (5, 1),\n",
      "  (6, 1),\n",
      "  (7, 1),\n",
      "  (8, 1),\n",
      "  (9, 2),\n",
      "  (10, 1),\n",
      "  (11, 3),\n",
      "  (12, 1),\n",
      "  (13, 4),\n",
      "  (14, 1),\n",
      "  (15, 1),\n",
      "  (16, 1),\n",
      "  (17, 1),\n",
      "  (18, 2),\n",
      "  (19, 4),\n",
      "  (20, 1),\n",
      "  (21, 1),\n",
      "  (22, 2),\n",
      "  (23, 1),\n",
      "  (24, 1),\n",
      "  (25, 1),\n",
      "  (26, 2),\n",
      "  (27, 1),\n",
      "  (28, 1),\n",
      "  (29, 3),\n",
      "  (30, 1),\n",
      "  (31, 1),\n",
      "  (32, 1),\n",
      "  (33, 1),\n",
      "  (34, 1),\n",
      "  (35, 1),\n",
      "  (36, 1),\n",
      "  (37, 1),\n",
      "  (38, 2),\n",
      "  (39, 1),\n",
      "  (40, 1),\n",
      "  (41, 5),\n",
      "  (42, 1),\n",
      "  (43, 1),\n",
      "  (44, 1),\n",
      "  (45, 2),\n",
      "  (46, 2),\n",
      "  (47, 2),\n",
      "  (48, 1),\n",
      "  (49, 1),\n",
      "  (50, 2),\n",
      "  (51, 3),\n",
      "  (52, 2),\n",
      "  (53, 1),\n",
      "  (54, 2),\n",
      "  (55, 1),\n",
      "  (56, 5),\n",
      "  (57, 1),\n",
      "  (58, 5),\n",
      "  (59, 1),\n",
      "  (60, 1),\n",
      "  (61, 1),\n",
      "  (62, 1),\n",
      "  (63, 3),\n",
      "  (64, 1),\n",
      "  (65, 2),\n",
      "  (66, 1),\n",
      "  (67, 2),\n",
      "  (68, 4),\n",
      "  (69, 1),\n",
      "  (70, 1),\n",
      "  (71, 1),\n",
      "  (72, 2),\n",
      "  (73, 2),\n",
      "  (74, 1),\n",
      "  (75, 1)],\n",
      " [(43, 1),\n",
      "  (45, 1),\n",
      "  (76, 2),\n",
      "  (77, 1),\n",
      "  (78, 1),\n",
      "  (79, 3),\n",
      "  (80, 1),\n",
      "  (81, 1),\n",
      "  (82, 1),\n",
      "  (83, 2),\n",
      "  (84, 1),\n",
      "  (85, 4),\n",
      "  (86, 1),\n",
      "  (87, 1),\n",
      "  (88, 1),\n",
      "  (89, 1),\n",
      "  (90, 1),\n",
      "  (91, 1),\n",
      "  (92, 1),\n",
      "  (93, 1),\n",
      "  (94, 1),\n",
      "  (95, 1),\n",
      "  (96, 3),\n",
      "  (97, 2),\n",
      "  (98, 1),\n",
      "  (99, 3),\n",
      "  (100, 2),\n",
      "  (101, 1),\n",
      "  (102, 1),\n",
      "  (103, 1),\n",
      "  (104, 1),\n",
      "  (105, 1),\n",
      "  (106, 1),\n",
      "  (107, 6),\n",
      "  (108, 1),\n",
      "  (109, 1),\n",
      "  (110, 1),\n",
      "  (111, 3),\n",
      "  (112, 1),\n",
      "  (113, 1),\n",
      "  (114, 1),\n",
      "  (115, 1),\n",
      "  (116, 3),\n",
      "  (117, 2),\n",
      "  (118, 2),\n",
      "  (119, 1),\n",
      "  (120, 1),\n",
      "  (121, 1),\n",
      "  (122, 2),\n",
      "  (123, 2),\n",
      "  (124, 2),\n",
      "  (125, 2),\n",
      "  (126, 2),\n",
      "  (127, 2),\n",
      "  (128, 3),\n",
      "  (129, 3),\n",
      "  (130, 1),\n",
      "  (131, 1),\n",
      "  (132, 3),\n",
      "  (133, 1),\n",
      "  (134, 1),\n",
      "  (135, 1),\n",
      "  (136, 7),\n",
      "  (137, 1),\n",
      "  (138, 2),\n",
      "  (139, 1),\n",
      "  (140, 1),\n",
      "  (141, 1),\n",
      "  (142, 1),\n",
      "  (143, 1),\n",
      "  (144, 1),\n",
      "  (145, 1),\n",
      "  (146, 1),\n",
      "  (147, 1),\n",
      "  (148, 10),\n",
      "  (149, 2),\n",
      "  (150, 1),\n",
      "  (151, 2),\n",
      "  (152, 1),\n",
      "  (153, 2),\n",
      "  (154, 1),\n",
      "  (155, 1),\n",
      "  (156, 1),\n",
      "  (157, 1),\n",
      "  (158, 9),\n",
      "  (159, 3),\n",
      "  (160, 1),\n",
      "  (161, 2),\n",
      "  (162, 7),\n",
      "  (163, 2),\n",
      "  (164, 4),\n",
      "  (165, 1),\n",
      "  (166, 1),\n",
      "  (167, 1),\n",
      "  (168, 1),\n",
      "  (169, 2),\n",
      "  (170, 1),\n",
      "  (171, 1),\n",
      "  (172, 1),\n",
      "  (173, 20),\n",
      "  (174, 1),\n",
      "  (175, 1),\n",
      "  (176, 1),\n",
      "  (177, 4),\n",
      "  (178, 1),\n",
      "  (179, 3),\n",
      "  (180, 1),\n",
      "  (181, 1),\n",
      "  (182, 1),\n",
      "  (183, 1),\n",
      "  (184, 2),\n",
      "  (185, 2),\n",
      "  (186, 2),\n",
      "  (187, 1),\n",
      "  (188, 3),\n",
      "  (189, 1),\n",
      "  (190, 1),\n",
      "  (191, 1),\n",
      "  (192, 1),\n",
      "  (193, 1),\n",
      "  (194, 1),\n",
      "  (195, 1),\n",
      "  (196, 1),\n",
      "  (197, 1),\n",
      "  (198, 1),\n",
      "  (199, 1),\n",
      "  (200, 1),\n",
      "  (201, 1),\n",
      "  (202, 1),\n",
      "  (203, 1),\n",
      "  (204, 1),\n",
      "  (205, 2),\n",
      "  (206, 1),\n",
      "  (207, 7),\n",
      "  (208, 1),\n",
      "  (209, 1),\n",
      "  (210, 2),\n",
      "  (211, 1),\n",
      "  (212, 3),\n",
      "  (213, 4),\n",
      "  (214, 1),\n",
      "  (215, 7),\n",
      "  (216, 1),\n",
      "  (217, 1),\n",
      "  (218, 1),\n",
      "  (219, 1),\n",
      "  (220, 1),\n",
      "  (221, 1),\n",
      "  (222, 1),\n",
      "  (223, 1),\n",
      "  (224, 2),\n",
      "  (225, 1),\n",
      "  (226, 3),\n",
      "  (227, 1),\n",
      "  (228, 1),\n",
      "  (229, 1),\n",
      "  (230, 1),\n",
      "  (231, 3),\n",
      "  (232, 2),\n",
      "  (233, 1),\n",
      "  (234, 2),\n",
      "  (235, 3)]]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(corpus[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-18 09:23:54,577 : WARNING : scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "2020-06-18 09:23:55,075 : INFO : creating matrix with 1005 documents and 11105 features\n"
     ]
    }
   ],
   "source": [
    "index = similarities.MatrixSimilarity(tf_idf[corpus])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 0.8459525\n",
      "483 0.11595668\n",
      "653 0.11566185\n",
      "443 0.08606418\n",
      "622 0.0748373\n",
      "298 0.07418704\n",
      "339 0.071005516\n",
      "327 0.06818734\n",
      "371 0.06796195\n",
      "315 0.06487929\n",
      "348 0.06225711\n",
      "956 0.059957404\n",
      "165 0.05980962\n",
      "996 0.054670077\n",
      "778 0.053686388\n",
      "82 0.052728444\n",
      "597 0.051916204\n",
      "751 0.051364847\n",
      "707 0.043157633\n",
      "846 0.041981574\n",
      "580 0.03954599\n",
      "788 0.038054198\n",
      "296 0.03713871\n",
      "36 0.036963742\n",
      "879 0.0368164\n",
      "21 0.036739632\n",
      "440 0.035467554\n",
      "953 0.033900443\n",
      "35 0.033481337\n",
      "913 0.033455756\n",
      "271 0.03276166\n",
      "221 0.032736387\n",
      "334 0.03246125\n",
      "507 0.03174052\n",
      "499 0.031045455\n",
      "797 0.031032456\n",
      "311 0.030903686\n",
      "312 0.030280098\n",
      "680 0.029878343\n",
      "461 0.029806994\n",
      "116 0.029659348\n",
      "148 0.029653452\n",
      "243 0.029493464\n",
      "113 0.028947107\n",
      "816 0.02846191\n",
      "46 0.028146721\n",
      "669 0.027783422\n",
      "292 0.027584933\n",
      "9 0.027326383\n",
      "520 0.027126888\n",
      "3 0.026542025\n",
      "233 0.02637478\n",
      "65 0.025726298\n",
      "943 0.025676448\n",
      "399 0.025495697\n",
      "807 0.025269594\n",
      "414 0.025067756\n",
      "344 0.024766719\n",
      "532 0.024764955\n",
      "269 0.024758764\n",
      "44 0.02407913\n",
      "88 0.02406754\n",
      "976 0.024049094\n",
      "761 0.023970228\n",
      "505 0.023898652\n",
      "231 0.023809776\n",
      "808 0.023807742\n",
      "931 0.02372852\n",
      "766 0.023541689\n",
      "306 0.02347046\n",
      "682 0.023441236\n",
      "952 0.0231792\n",
      "702 0.023097375\n",
      "955 0.023058642\n",
      "925 0.022994846\n",
      "742 0.022980813\n",
      "932 0.022813337\n",
      "905 0.022527184\n",
      "538 0.021580994\n",
      "1003 0.02149614\n",
      "1004 0.021429028\n",
      "848 0.020818498\n",
      "548 0.020796688\n",
      "450 0.020754026\n",
      "264 0.020428648\n",
      "33 0.020267164\n",
      "713 0.020044\n",
      "540 0.019856654\n",
      "71 0.01980493\n",
      "997 0.019798197\n",
      "710 0.019740097\n",
      "176 0.01957261\n",
      "512 0.019329555\n",
      "442 0.019019343\n",
      "709 0.018791975\n",
      "784 0.018584918\n",
      "777 0.018281804\n",
      "413 0.018274926\n",
      "894 0.018239176\n",
      "740 0.01820739\n",
      "488 0.018160911\n",
      "985 0.018061591\n",
      "914 0.01801332\n",
      "861 0.017973652\n",
      "688 0.017973125\n",
      "390 0.017900769\n",
      "673 0.017754335\n",
      "690 0.017531276\n",
      "83 0.017467303\n",
      "370 0.017463777\n",
      "487 0.017444469\n",
      "368 0.0174179\n",
      "631 0.017387705\n",
      "573 0.017350443\n",
      "741 0.017317902\n",
      "659 0.01728392\n",
      "26 0.017266266\n",
      "434 0.016987707\n",
      "860 0.016949467\n",
      "486 0.01692566\n",
      "358 0.016773654\n",
      "836 0.016687304\n",
      "521 0.016615361\n",
      "747 0.016524037\n",
      "833 0.016478905\n",
      "717 0.016446438\n",
      "526 0.016418846\n",
      "817 0.016364627\n",
      "93 0.016340867\n",
      "880 0.016265165\n",
      "617 0.016165812\n",
      "209 0.01616314\n",
      "338 0.016133413\n",
      "1001 0.016018962\n",
      "511 0.015998857\n",
      "530 0.015861586\n",
      "266 0.015802983\n",
      "780 0.015781725\n",
      "980 0.015748827\n",
      "940 0.015576841\n",
      "867 0.015564129\n",
      "830 0.01554567\n",
      "856 0.015503338\n",
      "366 0.0152174225\n",
      "890 0.015198754\n",
      "661 0.015194453\n",
      "156 0.015182415\n",
      "899 0.015145569\n",
      "372 0.015125547\n",
      "854 0.015098384\n",
      "599 0.015080191\n",
      "48 0.015022256\n",
      "719 0.01497641\n",
      "853 0.014952242\n",
      "589 0.014952017\n",
      "403 0.014813819\n",
      "918 0.014741383\n",
      "942 0.014721248\n",
      "965 0.014647956\n",
      "377 0.014570211\n",
      "972 0.014402019\n",
      "280 0.014375436\n",
      "504 0.014302092\n",
      "609 0.014251909\n",
      "989 0.014119087\n",
      "839 0.01409388\n",
      "95 0.014034981\n",
      "208 0.014016345\n",
      "884 0.013922916\n",
      "799 0.013898806\n",
      "546 0.013889423\n",
      "887 0.013754347\n",
      "145 0.013719084\n",
      "61 0.01371739\n",
      "62 0.01371739\n",
      "970 0.013692651\n",
      "30 0.013662741\n",
      "849 0.01365783\n",
      "397 0.013638547\n",
      "536 0.013493676\n",
      "706 0.01346356\n",
      "764 0.013435889\n",
      "119 0.013419284\n",
      "73 0.01340176\n",
      "1002 0.013380004\n",
      "693 0.013368126\n",
      "102 0.013358718\n",
      "343 0.013357003\n",
      "666 0.013332015\n",
      "757 0.013305471\n",
      "961 0.01329832\n",
      "528 0.013244494\n",
      "550 0.013241175\n",
      "835 0.013202149\n",
      "531 0.013201799\n",
      "259 0.013183735\n",
      "559 0.0131438\n",
      "606 0.01313887\n",
      "172 0.013084939\n",
      "360 0.013080364\n",
      "657 0.013067212\n",
      "864 0.012920048\n",
      "993 0.012799831\n",
      "963 0.012798132\n",
      "821 0.012771011\n",
      "436 0.012744692\n",
      "439 0.01267063\n",
      "692 0.012666022\n",
      "634 0.012665004\n",
      "190 0.012642583\n",
      "529 0.01255367\n",
      "321 0.012551552\n",
      "954 0.012530599\n",
      "863 0.012529839\n",
      "391 0.012492756\n",
      "326 0.012483796\n",
      "916 0.012431432\n",
      "341 0.0123903565\n",
      "838 0.012359274\n",
      "373 0.012334401\n",
      "419 0.012268011\n",
      "227 0.012266483\n",
      "801 0.012261319\n",
      "847 0.012243242\n",
      "746 0.012222341\n",
      "687 0.012194522\n",
      "771 0.012176327\n",
      "900 0.012101294\n",
      "299 0.012027308\n",
      "832 0.011940178\n",
      "293 0.011933926\n",
      "463 0.011859075\n",
      "581 0.011824661\n",
      "969 0.011779866\n",
      "722 0.011768014\n",
      "398 0.011711538\n",
      "345 0.011696376\n",
      "154 0.011679633\n",
      "132 0.01166601\n",
      "361 0.011664896\n",
      "187 0.011576353\n",
      "715 0.011536887\n",
      "217 0.011529416\n",
      "704 0.011498392\n",
      "758 0.0114973225\n",
      "678 0.011470043\n",
      "862 0.011458181\n",
      "350 0.011428165\n",
      "285 0.0114188\n",
      "703 0.01141705\n",
      "910 0.011403209\n",
      "919 0.011394861\n",
      "696 0.011349998\n",
      "705 0.011349713\n",
      "588 0.011301595\n",
      "251 0.011293361\n",
      "549 0.011288628\n",
      "727 0.011280096\n",
      "873 0.011267615\n",
      "254 0.011247331\n",
      "857 0.011226416\n",
      "844 0.0112186745\n",
      "365 0.011187472\n",
      "728 0.011147689\n",
      "768 0.011083928\n",
      "576 0.011077002\n",
      "265 0.010994272\n",
      "643 0.010971654\n",
      "340 0.010946067\n",
      "566 0.010918375\n",
      "614 0.01090236\n",
      "630 0.010871982\n",
      "240 0.010811069\n",
      "554 0.010798652\n",
      "481 0.010751223\n",
      "960 0.010722283\n",
      "689 0.010721899\n",
      "349 0.010721507\n",
      "819 0.010663389\n",
      "514 0.010653119\n",
      "163 0.0106416205\n",
      "754 0.010619136\n",
      "628 0.010610791\n",
      "933 0.010574289\n",
      "396 0.010516328\n",
      "347 0.010514189\n",
      "781 0.010506168\n",
      "668 0.010476142\n",
      "63 0.010470888\n",
      "991 0.010467746\n",
      "823 0.01041852\n",
      "881 0.010409361\n",
      "885 0.010408218\n",
      "11 0.010333599\n",
      "944 0.010308301\n",
      "489 0.010273928\n",
      "855 0.010253893\n",
      "567 0.010196964\n",
      "624 0.010195965\n",
      "500 0.010189418\n",
      "19 0.010169617\n",
      "42 0.010168005\n",
      "524 0.010167408\n",
      "926 0.010139652\n",
      "519 0.0101251025\n",
      "804 0.010078773\n",
      "587 0.010073828\n",
      "577 0.0100366\n",
      "523 0.010021497\n",
      "915 0.009944167\n",
      "462 0.009879185\n",
      "866 0.009859978\n",
      "936 0.009858449\n",
      "552 0.009834217\n",
      "585 0.009831471\n",
      "101 0.009761919\n",
      "412 0.00972664\n",
      "80 0.009726321\n",
      "572 0.009723239\n",
      "181 0.009712976\n",
      "242 0.009706004\n",
      "551 0.009704122\n",
      "229 0.009670564\n",
      "0 0.009649085\n",
      "616 0.009646751\n",
      "603 0.009622529\n",
      "598 0.009585792\n",
      "255 0.009581497\n",
      "409 0.009547256\n",
      "401 0.009539882\n",
      "146 0.009522265\n",
      "60 0.009483869\n",
      "230 0.0094232755\n",
      "684 0.009417028\n",
      "583 0.009413887\n",
      "133 0.009403328\n",
      "582 0.009349246\n",
      "640 0.009343126\n",
      "518 0.009330954\n",
      "968 0.009308417\n",
      "814 0.009303827\n",
      "241 0.009295461\n",
      "449 0.009285266\n",
      "515 0.0092697805\n",
      "889 0.009266546\n",
      "875 0.009245661\n",
      "695 0.009232702\n",
      "615 0.009203902\n",
      "335 0.009158696\n",
      "408 0.009112502\n",
      "605 0.009098757\n",
      "502 0.009093358\n",
      "917 0.009083634\n",
      "638 0.009060083\n",
      "525 0.0090582\n",
      "294 0.008989903\n",
      "595 0.008972568\n",
      "430 0.008971303\n",
      "564 0.008959252\n",
      "144 0.008953859\n",
      "425 0.008952953\n",
      "565 0.008948847\n",
      "731 0.008938251\n",
      "270 0.008902209\n",
      "649 0.0088655185\n",
      "738 0.008848865\n",
      "928 0.008837783\n",
      "825 0.008800756\n",
      "627 0.008714484\n",
      "328 0.008645779\n",
      "422 0.008629123\n",
      "16 0.008619916\n",
      "683 0.008605301\n",
      "783 0.008602429\n",
      "736 0.008547645\n",
      "386 0.008541383\n",
      "981 0.008510889\n",
      "604 0.008508976\n",
      "496 0.008472647\n",
      "173 0.008424825\n",
      "827 0.008412743\n",
      "613 0.008412032\n",
      "962 0.00841076\n",
      "594 0.008402931\n",
      "639 0.008402475\n",
      "951 0.008396983\n",
      "602 0.008371148\n",
      "47 0.008368339\n",
      "647 0.008324591\n",
      "513 0.008282755\n",
      "104 0.008279538\n",
      "301 0.008264467\n",
      "124 0.00825081\n",
      "686 0.008238599\n",
      "575 0.008237029\n",
      "824 0.008202985\n",
      "635 0.0081972685\n",
      "645 0.008157938\n",
      "792 0.0081565585\n",
      "858 0.0081421705\n",
      "636 0.008134548\n",
      "281 0.0080985\n",
      "785 0.008096265\n",
      "681 0.008076099\n",
      "282 0.008052425\n",
      "553 0.008048839\n",
      "498 0.008042469\n",
      "618 0.007994236\n",
      "612 0.007989551\n",
      "533 0.007960616\n",
      "957 0.007956643\n",
      "570 0.007956249\n",
      "129 0.007951501\n",
      "896 0.007927109\n",
      "662 0.0078957705\n",
      "485 0.007863728\n",
      "770 0.007806622\n",
      "4 0.007766793\n",
      "310 0.0077384263\n",
      "986 0.0077301394\n",
      "924 0.007713144\n",
      "322 0.0076818336\n",
      "121 0.007611906\n",
      "205 0.0075717685\n",
      "815 0.0075001246\n",
      "964 0.007449654\n",
      "52 0.0074471817\n",
      "891 0.007422067\n",
      "417 0.007416761\n",
      "623 0.0073561645\n",
      "407 0.0072876518\n",
      "460 0.0072302287\n",
      "469 0.007160329\n",
      "822 0.007137125\n",
      "813 0.0071221227\n",
      "563 0.007116181\n",
      "663 0.0071028583\n",
      "590 0.0070685195\n",
      "626 0.007067938\n",
      "753 0.0070458916\n",
      "592 0.0070158364\n",
      "911 0.0070115356\n",
      "557 0.0070115235\n",
      "922 0.006968905\n",
      "497 0.0069375522\n",
      "759 0.006924753\n",
      "608 0.0068926797\n",
      "958 0.0068485364\n",
      "541 0.006831037\n",
      "574 0.0068216366\n",
      "675 0.0068162433\n",
      "679 0.006792592\n",
      "454 0.006754024\n",
      "904 0.0067271683\n",
      "457 0.00670146\n",
      "677 0.0066794567\n",
      "224 0.006666186\n",
      "509 0.0066404324\n",
      "406 0.0065725734\n",
      "55 0.0065622968\n",
      "122 0.0065607694\n",
      "882 0.0064407927\n",
      "100 0.0064406404\n",
      "128 0.006434063\n",
      "79 0.006420886\n",
      "584 0.0063905986\n",
      "674 0.0063899355\n",
      "2 0.0063714096\n",
      "990 0.0063435156\n",
      "109 0.0063255876\n",
      "152 0.0062954756\n",
      "721 0.0062810844\n",
      "174 0.006269386\n",
      "755 0.00623268\n",
      "611 0.0061950553\n",
      "455 0.0061650397\n",
      "465 0.0061564688\n",
      "672 0.0060952576\n",
      "169 0.0060888515\n",
      "656 0.006014951\n",
      "313 0.0059259157\n",
      "748 0.005884161\n",
      "998 0.005856491\n",
      "596 0.0058496464\n",
      "859 0.0058314074\n",
      "948 0.005821185\n",
      "999 0.0058161486\n",
      "355 0.005814138\n",
      "10 0.0058077034\n",
      "141 0.005778323\n",
      "427 0.005773965\n",
      "58 0.005734167\n",
      "516 0.005719\n",
      "648 0.0056819734\n",
      "971 0.0055952827\n",
      "228 0.005570539\n",
      "161 0.0055598263\n",
      "929 0.0055436697\n",
      "607 0.005531485\n",
      "620 0.005497104\n",
      "236 0.0054686433\n",
      "503 0.0054509356\n",
      "268 0.0054332027\n",
      "12 0.0054197726\n",
      "730 0.005398583\n",
      "158 0.005375438\n",
      "776 0.005339105\n",
      "841 0.005331205\n",
      "851 0.005314352\n",
      "167 0.0053026974\n",
      "641 0.0052983896\n",
      "389 0.0052921833\n",
      "319 0.0052573713\n",
      "153 0.005241195\n",
      "431 0.0052403286\n",
      "130 0.005201329\n",
      "479 0.0051922337\n",
      "410 0.005190413\n",
      "752 0.0051845233\n",
      "76 0.0051617436\n",
      "385 0.005049765\n",
      "545 0.005042947\n",
      "27 0.005038873\n",
      "131 0.0050342297\n",
      "210 0.005006934\n",
      "650 0.0049896333\n",
      "966 0.0049822815\n",
      "506 0.0049413256\n",
      "820 0.0049302294\n",
      "23 0.0049231583\n",
      "459 0.004916173\n",
      "423 0.004910496\n",
      "786 0.004878233\n",
      "664 0.004878193\n",
      "290 0.004845651\n",
      "90 0.0048331213\n",
      "779 0.0048110485\n",
      "250 0.0047674677\n",
      "151 0.0047592074\n",
      "632 0.004734121\n",
      "850 0.004712605\n",
      "421 0.0047042174\n",
      "364 0.0046972474\n",
      "433 0.0046389205\n",
      "139 0.004565962\n",
      "694 0.004561889\n",
      "876 0.0045396937\n",
      "495 0.004523829\n",
      "763 0.004503211\n",
      "874 0.004480455\n",
      "544 0.004459998\n",
      "354 0.0044591273\n",
      "7 0.0044574495\n",
      "186 0.0044574416\n",
      "252 0.004445703\n",
      "245 0.00441138\n",
      "484 0.0044072354\n",
      "105 0.0043817065\n",
      "895 0.0043610577\n",
      "256 0.0043150517\n",
      "262 0.0042880983\n",
      "137 0.0042849323\n",
      "743 0.00428099\n",
      "472 0.004255399\n",
      "429 0.0042394116\n",
      "878 0.0042163352\n",
      "789 0.0042107645\n",
      "54 0.004189565\n",
      "393 0.0041449945\n",
      "272 0.0041309395\n",
      "160 0.0041262144\n",
      "517 0.00411701\n",
      "476 0.004090421\n",
      "283 0.0040825447\n",
      "267 0.004056112\n",
      "278 0.004004473\n",
      "192 0.003984633\n",
      "464 0.003953032\n",
      "446 0.0039327275\n",
      "983 0.0039012926\n",
      "115 0.0038683768\n",
      "725 0.0038666313\n",
      "43 0.003865763\n",
      "556 0.003865382\n",
      "193 0.0038591265\n",
      "676 0.0038124218\n",
      "1 0.0038100707\n",
      "363 0.0038099303\n",
      "277 0.0037851725\n",
      "558 0.0037780907\n",
      "253 0.0037580235\n",
      "652 0.003732406\n",
      "103 0.0037062734\n",
      "126 0.0036957767\n",
      "729 0.0036712836\n",
      "805 0.003669937\n",
      "260 0.0036533587\n",
      "987 0.0036167665\n",
      "775 0.0036118678\n",
      "286 0.0035672493\n",
      "219 0.0035509996\n",
      "798 0.0035448708\n",
      "543 0.0035255312\n",
      "155 0.0035233214\n",
      "353 0.0034820277\n",
      "22 0.0034768975\n",
      "5 0.003472436\n",
      "735 0.0034480053\n",
      "184 0.003444627\n",
      "888 0.0034315214\n",
      "287 0.003430458\n",
      "56 0.0034231408\n",
      "420 0.0033848903\n",
      "127 0.0033750653\n",
      "387 0.0033730832\n",
      "51 0.0033568237\n",
      "84 0.003345826\n",
      "834 0.0033379716\n",
      "912 0.003319397\n",
      "388 0.0033181347\n",
      "31 0.0033096331\n",
      "149 0.0032620942\n",
      "69 0.0032379199\n",
      "41 0.0031951813\n",
      "237 0.0031764477\n",
      "542 0.0031687487\n",
      "317 0.0031056353\n",
      "642 0.0030981642\n",
      "244 0.0030277558\n",
      "49 0.0030139692\n",
      "331 0.0030126313\n",
      "437 0.0029926454\n",
      "108 0.0029756161\n",
      "840 0.0029584388\n",
      "732 0.0029442534\n",
      "367 0.002914314\n",
      "939 0.0029131917\n",
      "247 0.0029094606\n",
      "945 0.002895289\n",
      "336 0.0028944903\n",
      "45 0.0028874963\n",
      "671 0.0028292544\n",
      "142 0.002817423\n",
      "750 0.002815305\n",
      "162 0.00279694\n",
      "140 0.0027541583\n",
      "356 0.00273746\n",
      "147 0.0027200244\n",
      "150 0.002719326\n",
      "865 0.0027159525\n",
      "982 0.0027085466\n",
      "206 0.002666999\n",
      "658 0.0026323819\n",
      "330 0.0026253955\n",
      "492 0.002606239\n",
      "586 0.0025098068\n",
      "197 0.0025000728\n",
      "374 0.0024794498\n",
      "53 0.002468431\n",
      "426 0.0024666125\n",
      "453 0.0024599172\n",
      "166 0.0024421825\n",
      "468 0.002412156\n",
      "297 0.0024028076\n",
      "70 0.002380227\n",
      "314 0.0023536903\n",
      "791 0.0023331824\n",
      "769 0.0023314985\n",
      "491 0.002329038\n",
      "670 0.0023189043\n",
      "787 0.0023161133\n",
      "445 0.002298372\n",
      "274 0.002286942\n",
      "302 0.0022832486\n",
      "89 0.0022715619\n",
      "8 0.0022705114\n",
      "934 0.00227018\n",
      "211 0.002258475\n",
      "94 0.0022377186\n",
      "800 0.002148398\n",
      "342 0.002078328\n",
      "214 0.0020779371\n",
      "475 0.0020657657\n",
      "537 0.0020656253\n",
      "20 0.0020523707\n",
      "98 0.0020358423\n",
      "452 0.0020283377\n",
      "892 0.0020213171\n",
      "157 0.0020118777\n",
      "967 0.0019782488\n",
      "284 0.0019668352\n",
      "938 0.0019566887\n",
      "921 0.0019344684\n",
      "195 0.0019256156\n",
      "651 0.0019176198\n",
      "490 0.001904412\n",
      "199 0.0018873841\n",
      "828 0.0018482788\n",
      "232 0.0018367867\n",
      "712 0.0018331485\n",
      "749 0.001832106\n",
      "198 0.001830885\n",
      "318 0.0018250365\n",
      "774 0.0018186594\n",
      "110 0.0018008497\n",
      "767 0.0018004263\n",
      "701 0.0017975513\n",
      "868 0.0017962338\n",
      "135 0.0017881914\n",
      "601 0.0017675895\n",
      "941 0.0017608977\n",
      "984 0.001751539\n",
      "466 0.0017481212\n",
      "432 0.001728366\n",
      "324 0.0017255399\n",
      "138 0.0016944709\n",
      "275 0.0016459025\n",
      "424 0.0016357431\n",
      "337 0.0016302641\n",
      "188 0.0016233899\n",
      "979 0.0015982292\n",
      "379 0.0015931623\n",
      "207 0.0015884459\n",
      "14 0.0015814984\n",
      "923 0.0015794885\n",
      "106 0.0015740399\n",
      "745 0.0015427998\n",
      "927 0.0015270334\n",
      "478 0.0015250994\n",
      "405 0.0015108287\n",
      "471 0.0015104233\n",
      "547 0.0014990019\n",
      "74 0.001486562\n",
      "346 0.0014798379\n",
      "482 0.0014744094\n",
      "920 0.0014736357\n",
      "225 0.001471519\n",
      "273 0.001454456\n",
      "59 0.0014517066\n",
      "625 0.0014441538\n",
      "418 0.0014432354\n",
      "527 0.001429357\n",
      "362 0.0014239879\n",
      "309 0.0014029072\n",
      "633 0.0014002643\n",
      "57 0.0013961066\n",
      "178 0.0013792826\n",
      "357 0.0013756022\n",
      "177 0.0013740096\n",
      "718 0.0013551873\n",
      "415 0.0013515367\n",
      "194 0.001350231\n",
      "893 0.0013414477\n",
      "435 0.0013389054\n",
      "493 0.0013373451\n",
      "803 0.0013251279\n",
      "843 0.0013050793\n",
      "303 0.0012884683\n",
      "202 0.0012839837\n",
      "441 0.0012220298\n",
      "973 0.0012197577\n",
      "726 0.0012144434\n",
      "660 0.0011745307\n",
      "392 0.0011643796\n",
      "248 0.0011605185\n",
      "223 0.0011591692\n",
      "279 0.0011503374\n",
      "416 0.0011390452\n",
      "216 0.0010981843\n",
      "871 0.0010650619\n",
      "724 0.0010245664\n",
      "467 0.0010017378\n",
      "182 0.000994692\n",
      "950 0.0009917247\n",
      "646 0.0009742746\n",
      "329 0.00096776086\n",
      "501 0.0009351134\n",
      "307 0.000906498\n",
      "215 0.0009037012\n",
      "234 0.00090357166\n",
      "323 0.0008700766\n",
      "644 0.0008621494\n",
      "811 0.0008592951\n",
      "534 0.00085428223\n",
      "24 0.00083704997\n",
      "180 0.0007815434\n",
      "380 0.0007724102\n",
      "238 0.00076672714\n",
      "508 0.0007572883\n",
      "222 0.0007382118\n",
      "906 0.0007294449\n",
      "562 0.0007189209\n",
      "189 0.0007149594\n",
      "831 0.00071449386\n",
      "494 0.0006999379\n",
      "447 0.0006923318\n",
      "263 0.0006862055\n",
      "578 0.0006847422\n",
      "320 0.00067859393\n",
      "257 0.00067593163\n",
      "829 0.0006481547\n",
      "304 0.0006257933\n",
      "629 0.00062481756\n",
      "737 0.00061397627\n",
      "795 0.0006107188\n",
      "902 0.0006047864\n",
      "591 0.00059278816\n",
      "325 0.0005870134\n",
      "877 0.000580862\n",
      "535 0.0005752346\n",
      "610 0.00056208426\n",
      "382 0.00055876415\n",
      "402 0.00052844535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375 0.0005195068\n",
      "539 0.00050893123\n",
      "136 0.00050861406\n",
      "185 0.00050117535\n",
      "85 0.0004966631\n",
      "448 0.00049100374\n",
      "593 0.00048888347\n",
      "619 0.00048595903\n",
      "654 0.00047223162\n",
      "569 0.00046801148\n",
      "359 0.00046439696\n",
      "773 0.00045830297\n",
      "869 0.00045830297\n",
      "381 0.00045821778\n",
      "37 0.00044638012\n",
      "411 0.00043892744\n",
      "179 0.00043581787\n",
      "886 0.00043543766\n",
      "34 0.00041799727\n",
      "86 0.00041534693\n",
      "305 0.00040550923\n",
      "118 0.0004050799\n",
      "723 0.00040244625\n",
      "949 0.00039770408\n",
      "898 0.0003965405\n",
      "790 0.0003944041\n",
      "18 0.00038355426\n",
      "691 0.0003727919\n",
      "637 0.00037263276\n",
      "697 0.00036980928\n",
      "383 0.00036397978\n",
      "667 0.00036371217\n",
      "699 0.0003611552\n",
      "15 0.00036014098\n",
      "50 0.00035691564\n",
      "809 0.00035375968\n",
      "909 0.00035037467\n",
      "458 0.000347436\n",
      "456 0.00034456875\n",
      "117 0.00034066825\n",
      "81 0.00033561862\n",
      "376 0.000333753\n",
      "793 0.00033262017\n",
      "120 0.00032734388\n",
      "444 0.0003096916\n",
      "261 0.00029804817\n",
      "300 0.000297735\n",
      "38 0.00029638375\n",
      "560 0.00028908014\n",
      "351 0.0002851229\n",
      "579 0.0002824821\n",
      "288 0.000272343\n",
      "842 0.00027074537\n",
      "946 0.0002703358\n",
      "13 0.00027024018\n",
      "470 0.00026866028\n",
      "107 0.00025653117\n",
      "394 0.00025595518\n",
      "555 0.0002536813\n",
      "852 0.00025116059\n",
      "17 0.00023730786\n",
      "974 0.00023703865\n",
      "369 0.00023551309\n",
      "170 0.00023349968\n",
      "64 0.00023335085\n",
      "333 0.00023204913\n",
      "25 0.00021913453\n",
      "806 0.0002129376\n",
      "249 0.00020847394\n",
      "837 0.00020271921\n",
      "291 0.00020134455\n",
      "744 0.00019999804\n",
      "164 0.00019737973\n",
      "308 0.00019177604\n",
      "143 0.0001802579\n",
      "191 0.0001778425\n",
      "123 0.00017116735\n",
      "29 0.00017033303\n",
      "384 0.00016748825\n",
      "239 0.00016429498\n",
      "404 0.00015983131\n",
      "975 0.0001479518\n",
      "474 0.00014546103\n",
      "810 0.0001429547\n",
      "994 0.00014202166\n",
      "826 0.00014125904\n",
      "907 0.00014056358\n",
      "561 0.00013723742\n",
      "134 0.00013451403\n",
      "883 0.0001286793\n",
      "99 0.00012864731\n",
      "28 0.00012388604\n",
      "992 0.00011826202\n",
      "204 0.00011628934\n",
      "708 0.00010818026\n",
      "600 0.00010755598\n",
      "872 9.772619e-05\n",
      "258 9.362336e-05\n",
      "621 9.107958e-05\n",
      "428 8.6798915e-05\n",
      "473 8.4697036e-05\n",
      "316 7.441674e-05\n",
      "665 7.2510506e-05\n",
      "218 7.001735e-05\n",
      "352 6.9621834e-05\n",
      "183 6.9455724e-05\n",
      "72 6.6798064e-05\n",
      "947 6.546286e-05\n",
      "522 6.1778985e-05\n",
      "711 6.0309332e-05\n",
      "988 5.9339913e-05\n",
      "203 5.7587175e-05\n",
      "289 5.6371013e-05\n",
      "295 4.7911613e-05\n",
      "201 4.4506858e-05\n",
      "794 4.402955e-05\n",
      "480 2.8283757e-05\n",
      "937 2.325649e-05\n",
      "87 1.9892126e-05\n",
      "568 1.9154093e-05\n",
      "6 0.0\n",
      "32 0.0\n",
      "40 0.0\n",
      "66 0.0\n",
      "67 0.0\n",
      "68 0.0\n",
      "75 0.0\n",
      "77 0.0\n",
      "78 0.0\n",
      "91 0.0\n",
      "92 0.0\n",
      "96 0.0\n",
      "97 0.0\n",
      "111 0.0\n",
      "112 0.0\n",
      "114 0.0\n",
      "125 0.0\n",
      "159 0.0\n",
      "168 0.0\n",
      "171 0.0\n",
      "175 0.0\n",
      "196 0.0\n",
      "200 0.0\n",
      "212 0.0\n",
      "213 0.0\n",
      "220 0.0\n",
      "226 0.0\n",
      "235 0.0\n",
      "246 0.0\n",
      "276 0.0\n",
      "332 0.0\n",
      "378 0.0\n",
      "395 0.0\n",
      "400 0.0\n",
      "438 0.0\n",
      "451 0.0\n",
      "477 0.0\n",
      "510 0.0\n",
      "571 0.0\n",
      "655 0.0\n",
      "685 0.0\n",
      "698 0.0\n",
      "700 0.0\n",
      "714 0.0\n",
      "716 0.0\n",
      "720 0.0\n",
      "733 0.0\n",
      "734 0.0\n",
      "739 0.0\n",
      "756 0.0\n",
      "760 0.0\n",
      "762 0.0\n",
      "765 0.0\n",
      "772 0.0\n",
      "782 0.0\n",
      "796 0.0\n",
      "802 0.0\n",
      "812 0.0\n",
      "818 0.0\n",
      "845 0.0\n",
      "870 0.0\n",
      "897 0.0\n",
      "901 0.0\n",
      "903 0.0\n",
      "908 0.0\n",
      "930 0.0\n",
      "935 0.0\n",
      "959 0.0\n",
      "977 0.0\n",
      "978 0.0\n",
      "995 0.0\n",
      "1000 0.0\n"
     ]
    }
   ],
   "source": [
    "query_document = \"金融 虎讯 月 日 消息 今日 菏泽市 地方 金融 监督 管理局 发布 该市 失联 小额贷款 公司 公告 显示 菏泽市 牡丹区 恒顺 小额贷款 有限公司 情形 监管 系统 或市 县 两级 地方 金融 监管部门 市场 监管部门 预留 电话 取得联系\".split()\n",
    "query_bow = dictionary.doc2bow(query_document)\n",
    "sims = index[tf_idf[query_bow]]\n",
    "for document_number, score in sorted(enumerate(sims), key=lambda x: x[1], reverse=True):\n",
    "    print(document_number, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "金融 虎讯 月 日 消息 今日 菏泽市 地方 金融 监督 管理局 发布 该市 失联 小额贷款 公司 公告 显示 菏泽市 牡丹区 恒顺 小额贷款 有限公司 情形 监管 系统 或市 县 两级 地方 金融 监管部门 市场 监管部门 预留 电话 取得联系 办公 场所 已转 做 长期 未向 监管 系统 报送 相关 数据 情形 小额贷款 公司 长期 脱离 监管 经营 情况 较大 风险 隐患 现 公告 请 牡丹区 恒顺 小额贷款 有限公司 公告 三十日 主动 该局 提供 相关 资料 情况 逾期 未 主动 山东省 地方 金融 条例 相关 进一步 监管 措施 返回 搜狐 查看 责任编辑\n"
     ]
    }
   ],
   "source": [
    "print(documents[39])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-18 09:23:55,907 : INFO : loading projection weights from ../data/sgns.financial.char.bz2\n",
      "2020-06-18 09:24:01,686 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-18 09:24:02,276 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-18 09:24:02,309 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-18 09:24:02,802 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-18 09:24:02,824 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-18 09:24:03,005 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-18 09:24:03,076 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-18 09:24:03,104 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-18 09:24:03,174 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-18 09:24:03,258 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-18 09:24:03,324 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-18 09:24:03,353 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-18 09:24:03,513 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-18 09:24:03,641 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-18 09:24:03,818 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-18 09:24:03,887 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-18 09:24:04,010 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-18 09:24:04,084 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-18 09:24:04,179 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-18 09:24:04,365 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-18 09:24:04,480 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-18 09:24:04,867 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-18 09:24:04,950 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-18 09:24:05,166 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-18 09:24:05,735 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-18 09:24:05,904 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-18 09:24:06,639 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-18 09:24:07,287 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-18 09:24:08,098 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-18 09:24:08,287 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-18 09:24:10,026 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-18 09:24:10,197 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-18 09:24:10,282 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-18 09:24:10,903 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-18 09:24:11,390 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-18 09:24:11,722 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-18 09:24:11,953 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-18 09:24:12,060 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-18 09:24:12,998 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-18 09:24:13,672 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-18 09:24:16,907 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-18 09:24:17,130 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-18 09:24:18,161 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-18 09:24:18,410 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-18 09:24:18,427 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-18 09:24:18,813 : WARNING : duplicate word '--------------------------------------------------------------------------------------------------' in word2vec file, ignoring all but first\n",
      "2020-06-18 09:24:19,515 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-18 09:24:23,044 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-18 09:27:09,684 : INFO : duplicate words detected, shrinking matrix size from 467389 to 467341\n",
      "2020-06-18 09:27:09,685 : INFO : loaded (467341, 300) matrix from ../data/sgns.financial.char.bz2\n"
     ]
    }
   ],
   "source": [
    "#加载预训练金融预料w2v model\n",
    "from gensim.models import KeyedVectors\n",
    "word_vectors_char = KeyedVectors.load_word2vec_format('../data/sgns.financial.char.bz2') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-18 09:27:09,693 : INFO : Removed 12 and 7 OOV words from document 1 and 2 (respectively).\n",
      "2020-06-18 09:27:09,694 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-06-18 09:27:09,696 : INFO : built Dictionary(496 unique tokens: ['CME', 'WTI', '一度', '一段时间', '下跌']...) from 2 documents (total 1150 corpus positions)\n",
      "2020-06-18 09:27:13,597 : INFO : Removed 12 and 24 OOV words from document 1 and 2 (respectively).\n",
      "2020-06-18 09:27:13,598 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-06-18 09:27:13,599 : INFO : built Dictionary(442 unique tokens: ['CME', 'WTI', '一度', '一段时间', '下跌']...) from 2 documents (total 850 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance = 3.4920\n",
      "distance = 5.2379\n"
     ]
    }
   ],
   "source": [
    "distance = word_vectors_char.wmdistance(texts[3], texts[44]) #两篇原油宝的文章\n",
    "print('distance = %.4f' % distance)\n",
    "distance = word_vectors_char.wmdistance(texts[3], texts[45]) #一篇原油宝一篇疫情\n",
    "print('distance = %.4f' % distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc2vec example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TaggedDocument(words=['新华社', '哥本哈根', '月', '日电', '记者', '林晶', '世界卫生组织', '欧洲', '区域', '办事处', '主任', '克卢格', '月', '日', '哥本哈根', '视频', '例会', '时', '呼吁', '区域', '各国', '特别', '社区', '传播', '控制', '国家', '应', '确保', '医疗卫生', '系统', '双轨制', '抗击', '新冠', '疫情', '保证', '常规', '医疗卫生', '服务', '运转', '克卢格', '说', '欧洲地区', '新冠', '疫情', '严峻', '一周', '累计', '确诊', '病例', '15%', '累计', '确诊', '病例', '达', '1408266', '例', '同期', '死亡', '病例', '18%', '累计', '死亡', '人数', '达', '129344', '欧洲地区', '累计', '确诊', '死亡', '病例', '占', '世界', '相关', '病例', '数', '46%', '63%', '克卢格', '呼吁', '欧洲各国', '政府', '卫生机构', '寻求', '办法', '控制', '新冠', '病毒', '社区', '传播', '前提', '快速', '恢复', '常规', '医疗卫生', '服务', '特别', '指出', '形势', '保证', '儿童', '接种', '麻疹', '常规', '疫苗', '重要性', '克卢格', '说', '新冠', '疫情', '短时期', '消失', '波', '第三', '波', '疫情', '认知', '动员', '社会', '理解', '协作', '双轨制', '医疗卫生', '系统', '保证', '应对', '新冠', '疫情', '反复', '时', '灵活性', '弹性'], tags=[0]), TaggedDocument(words=['昨晚', '美股', '率先', '突破', '站上', '日', '均线', '创新', '高', '东京', '日经指数', '涨幅', '超', '2%', '创新', '高', 'A股', '大板', '指', '高开高', '走', '上证', '一举', '拿下', '久攻不下', '2850', '点', '成交量', '暴增', '银行', '强', '银行', '板块', '昨天', '起到', '应有', '护盘', '作用', '轮', '证券', '板块', '启动', '大涨', '超', '2%', '上证指数', '支撑', '爆量', '上涨', '资金', '光速', '进场', '资金', '板块', '半导体', '板块', '板块', '指数', '暴涨', '7%', '创', '历史', '单日', '涨幅', '板块', '权重股', '行业龙头', '涨停', '如兆易', '创新', '通富', '微电', '长电', '科技', '沪', '硅', '产业', '板块', '效应', '板块', '效应', '吸引', '资金', '参与', '交易', '好事', '板块', '技术', '面', '短期', '均线', '均线', '粘合', '短期', '平均', '成本', '重叠', '爆量', '暴涨', '站', '上半年', '线', '近半年', '平均', '持仓', '成本', '获利', '人类', '趋利避害', '特性', '越', '赚钱', '越', '持股', '买入', '越', '亏损', '越', '卖出', '恐慌性', '吸引', '资金', '参与', '半导体', '指数', '无线耳机', '板块', '板块', '指数', '涨幅', '超', '5%', '佳禾', '智能', '天', '板', '市场', '龙头股', '华胜天', '成', '突破', '年', '高点', '涨停板', '亿', '资金', '抢筹', '封板', '突破', '趋势', '安洁', '科技', 'PE30', '倍', '估值', '不高', '流通', '市值', '亿', '年线', '启动', '早盘', '开盘', '分钟', '涨停', '速度', '之快', '前所未有', '资金', '心情', '急切', '可见一斑', '华胜天', '成', '科技股', '走势', '板块', '食品饮料', '农林牧渔', '人造肉', '医药', '板块', '究其原因', '逻辑', '是因为', '农林牧渔', '食品饮料', '人造肉', '医药', '消费', '板块', '偏', '防御', '指数', '一跌', '板块', '必涨', '这招', '屡试不爽', 'A股', '市场', '经验', '规律性', '质疑', '当作', '公理', '来记', '当作', '记住', '预计', '指数', '五一', '后先', '延续', '上涨', '态势', '科技股', '领涨', '交易日', '指数', '遇压', '调整', '指数', '调整', '防御性', '板块', '食品饮料', '农林牧渔', '人造肉', '医药', '复制', '前期', '行情', '下图', '农林牧渔', '板块', '指数', '上证指数', '叠加', '图', '上半', '农林牧渔', '板块', '指数', '上证指数', '走势', '农林牧渔', '上证指数', '叠加', '图', '透露', '观察', '市场', '情绪', '指标', '规律', '前天', '早盘', 'A股', '跌停', '数量', '上次', '跌停', '数量', '好巧', '巧合', '市场', '恐慌', '情绪', '聪明', '资金', '一看', '熟悉', '情况', '情绪', '低谷', '来临', '拐点', '到来', '久违', '百股', '涨停', '情绪', '时间', '周期', '情绪', '周期', '高点', '低点', '天', '时间', '低点', '高点', '天', '时间', '时间', '做', '防御性', '板块', '情绪', '低谷', '指标', '市场', '情绪', '退潮', '期时', '市场', '最先', '跌停', '数量', '增多', '连续', '跌停', '股', '数量', '增多', '恐慌性', '顶', '跌停', '数量', '只股', '跌停', '跌停', '数量', '减少', '涨停', '数量', '增多', '高潮', '指标', '充分利用', '市场', '情绪', '拐点', '情绪', '低点', '时', '布局', '情绪', '高潮', '规避', '转向', '防御性', '板块'], tags=[1])]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "train_corpus = []\n",
    "for i in range(n):\n",
    "    train_corpus.append(gensim.models.doc2vec.TaggedDocument(documents[i].split(), [i]))\n",
    "print(train_corpus[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-18 09:27:15,883 : INFO : collecting all words and their counts\n",
      "2020-06-18 09:27:15,884 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2020-06-18 09:27:15,926 : INFO : collected 35788 word types and 1000 unique tags from a corpus of 1000 examples and 230224 words\n",
      "2020-06-18 09:27:15,927 : INFO : Loading a fresh vocabulary\n",
      "2020-06-18 09:27:15,964 : INFO : effective_min_count=2 retains 16894 unique words (47% of original 35788, drops 18894)\n",
      "2020-06-18 09:27:15,966 : INFO : effective_min_count=2 leaves 211330 word corpus (91% of original 230224, drops 18894)\n",
      "2020-06-18 09:27:16,021 : INFO : deleting the raw counts dictionary of 35788 items\n",
      "2020-06-18 09:27:16,023 : INFO : sample=0.001 downsamples 18 most-common words\n",
      "2020-06-18 09:27:16,024 : INFO : downsampling leaves estimated 203913 word corpus (96.5% of prior 211330)\n",
      "2020-06-18 09:27:16,059 : INFO : estimated required memory for 16894 words and 20 dimensions: 11230040 bytes\n",
      "2020-06-18 09:27:16,060 : INFO : resetting layer weights\n",
      "2020-06-18 09:27:19,259 : INFO : training model with 3 workers on 16894 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-06-18 09:27:19,412 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-18 09:27:19,414 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-18 09:27:19,420 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-18 09:27:19,420 : INFO : EPOCH - 1 : training on 230224 raw words (204886 effective words) took 0.2s, 1325780 effective words/s\n",
      "2020-06-18 09:27:19,572 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-18 09:27:19,578 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-18 09:27:19,579 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-18 09:27:19,580 : INFO : EPOCH - 2 : training on 230224 raw words (204900 effective words) took 0.2s, 1299902 effective words/s\n",
      "2020-06-18 09:27:19,729 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-18 09:27:19,730 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-18 09:27:19,734 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-18 09:27:19,735 : INFO : EPOCH - 3 : training on 230224 raw words (204883 effective words) took 0.2s, 1334901 effective words/s\n",
      "2020-06-18 09:27:19,882 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-18 09:27:19,883 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-18 09:27:19,887 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-18 09:27:19,887 : INFO : EPOCH - 4 : training on 230224 raw words (204956 effective words) took 0.2s, 1361859 effective words/s\n",
      "2020-06-18 09:27:20,033 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-18 09:27:20,035 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-18 09:27:20,039 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-18 09:27:20,040 : INFO : EPOCH - 5 : training on 230224 raw words (204872 effective words) took 0.2s, 1359996 effective words/s\n",
      "2020-06-18 09:27:20,188 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-18 09:27:20,189 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-18 09:27:20,192 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-18 09:27:20,192 : INFO : EPOCH - 6 : training on 230224 raw words (204997 effective words) took 0.2s, 1356764 effective words/s\n",
      "2020-06-18 09:27:20,342 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-18 09:27:20,343 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-18 09:27:20,348 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-18 09:27:20,348 : INFO : EPOCH - 7 : training on 230224 raw words (204959 effective words) took 0.2s, 1330360 effective words/s\n",
      "2020-06-18 09:27:20,493 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-18 09:27:20,496 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-18 09:27:20,501 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-18 09:27:20,501 : INFO : EPOCH - 8 : training on 230224 raw words (204908 effective words) took 0.2s, 1357280 effective words/s\n",
      "2020-06-18 09:27:20,658 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-18 09:27:20,659 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-18 09:27:20,663 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-18 09:27:20,664 : INFO : EPOCH - 9 : training on 230224 raw words (205024 effective words) took 0.2s, 1278488 effective words/s\n",
      "2020-06-18 09:27:20,811 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-18 09:27:20,812 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-18 09:27:20,817 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-18 09:27:20,818 : INFO : EPOCH - 10 : training on 230224 raw words (204875 effective words) took 0.2s, 1347194 effective words/s\n",
      "2020-06-18 09:27:20,970 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-18 09:27:20,973 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-18 09:27:20,974 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-18 09:27:20,975 : INFO : EPOCH - 11 : training on 230224 raw words (204853 effective words) took 0.2s, 1321541 effective words/s\n",
      "2020-06-18 09:27:21,122 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-18 09:27:21,124 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-18 09:27:21,129 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-18 09:27:21,129 : INFO : EPOCH - 12 : training on 230224 raw words (204826 effective words) took 0.2s, 1340663 effective words/s\n",
      "2020-06-18 09:27:21,275 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-18 09:27:21,277 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-18 09:27:21,282 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-18 09:27:21,283 : INFO : EPOCH - 13 : training on 230224 raw words (204815 effective words) took 0.2s, 1348949 effective words/s\n",
      "2020-06-18 09:27:21,436 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-18 09:27:21,438 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-18 09:27:21,442 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-18 09:27:21,442 : INFO : EPOCH - 14 : training on 230224 raw words (204932 effective words) took 0.2s, 1306110 effective words/s\n",
      "2020-06-18 09:27:21,590 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-18 09:27:21,592 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-18 09:27:21,597 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-18 09:27:21,598 : INFO : EPOCH - 15 : training on 230224 raw words (205051 effective words) took 0.2s, 1330324 effective words/s\n",
      "2020-06-18 09:27:21,748 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-18 09:27:21,749 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-18 09:27:21,754 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-18 09:27:21,755 : INFO : EPOCH - 16 : training on 230224 raw words (204885 effective words) took 0.2s, 1322362 effective words/s\n",
      "2020-06-18 09:27:21,900 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-18 09:27:21,902 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-18 09:27:21,908 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-18 09:27:21,908 : INFO : EPOCH - 17 : training on 230224 raw words (204954 effective words) took 0.2s, 1351365 effective words/s\n",
      "2020-06-18 09:27:22,049 : INFO : worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-18 09:27:22,051 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-18 09:27:22,055 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-18 09:27:22,056 : INFO : EPOCH - 18 : training on 230224 raw words (205006 effective words) took 0.1s, 1415741 effective words/s\n",
      "2020-06-18 09:27:22,215 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-18 09:27:22,216 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-18 09:27:22,221 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-18 09:27:22,222 : INFO : EPOCH - 19 : training on 230224 raw words (204898 effective words) took 0.2s, 1248663 effective words/s\n",
      "2020-06-18 09:27:22,368 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-18 09:27:22,369 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-18 09:27:22,375 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-18 09:27:22,376 : INFO : EPOCH - 20 : training on 230224 raw words (204893 effective words) took 0.2s, 1339686 effective words/s\n",
      "2020-06-18 09:27:22,524 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-18 09:27:22,525 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-18 09:27:22,530 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-18 09:27:22,530 : INFO : EPOCH - 21 : training on 230224 raw words (204906 effective words) took 0.2s, 1353437 effective words/s\n",
      "2020-06-18 09:27:22,675 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-18 09:27:22,676 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-18 09:27:22,681 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-18 09:27:22,681 : INFO : EPOCH - 22 : training on 230224 raw words (204883 effective words) took 0.1s, 1373115 effective words/s\n",
      "2020-06-18 09:27:22,827 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-18 09:27:22,828 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-18 09:27:22,832 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-18 09:27:22,832 : INFO : EPOCH - 23 : training on 230224 raw words (204967 effective words) took 0.1s, 1373181 effective words/s\n",
      "2020-06-18 09:27:22,978 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-18 09:27:22,979 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-18 09:27:22,984 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-18 09:27:22,985 : INFO : EPOCH - 24 : training on 230224 raw words (204910 effective words) took 0.2s, 1359830 effective words/s\n",
      "2020-06-18 09:27:23,143 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-18 09:27:23,144 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-18 09:27:23,149 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-18 09:27:23,150 : INFO : EPOCH - 25 : training on 230224 raw words (204897 effective words) took 0.2s, 1257103 effective words/s\n",
      "2020-06-18 09:27:23,296 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-18 09:27:23,300 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-18 09:27:23,302 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-18 09:27:23,302 : INFO : EPOCH - 26 : training on 230224 raw words (204838 effective words) took 0.2s, 1363033 effective words/s\n",
      "2020-06-18 09:27:23,451 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-18 09:27:23,453 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-18 09:27:23,457 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-18 09:27:23,457 : INFO : EPOCH - 27 : training on 230224 raw words (204962 effective words) took 0.2s, 1340307 effective words/s\n",
      "2020-06-18 09:27:23,603 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-18 09:27:23,604 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-18 09:27:23,610 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-18 09:27:23,610 : INFO : EPOCH - 28 : training on 230224 raw words (204877 effective words) took 0.2s, 1355368 effective words/s\n",
      "2020-06-18 09:27:23,756 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-18 09:27:23,757 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-18 09:27:23,763 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-18 09:27:23,763 : INFO : EPOCH - 29 : training on 230224 raw words (204971 effective words) took 0.2s, 1362165 effective words/s\n",
      "2020-06-18 09:27:23,899 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-18 09:27:23,901 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-18 09:27:23,907 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-18 09:27:23,908 : INFO : EPOCH - 30 : training on 230224 raw words (204945 effective words) took 0.1s, 1436649 effective words/s\n",
      "2020-06-18 09:27:24,045 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-18 09:27:24,047 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-18 09:27:24,051 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-18 09:27:24,052 : INFO : EPOCH - 31 : training on 230224 raw words (204936 effective words) took 0.1s, 1455181 effective words/s\n",
      "2020-06-18 09:27:24,196 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-18 09:27:24,197 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-18 09:27:24,201 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-18 09:27:24,202 : INFO : EPOCH - 32 : training on 230224 raw words (204833 effective words) took 0.1s, 1377652 effective words/s\n",
      "2020-06-18 09:27:24,347 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-18 09:27:24,350 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-18 09:27:24,354 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-18 09:27:24,354 : INFO : EPOCH - 33 : training on 230224 raw words (204905 effective words) took 0.2s, 1360904 effective words/s\n",
      "2020-06-18 09:27:24,497 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-18 09:27:24,502 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-18 09:27:24,505 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-18 09:27:24,506 : INFO : EPOCH - 34 : training on 230224 raw words (204968 effective words) took 0.1s, 1369136 effective words/s\n",
      "2020-06-18 09:27:24,652 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-18 09:27:24,654 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-18 09:27:24,657 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-18 09:27:24,657 : INFO : EPOCH - 35 : training on 230224 raw words (204859 effective words) took 0.1s, 1372344 effective words/s\n",
      "2020-06-18 09:27:24,800 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-18 09:27:24,803 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-18 09:27:24,805 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-18 09:27:24,805 : INFO : EPOCH - 36 : training on 230224 raw words (204950 effective words) took 0.1s, 1402331 effective words/s\n",
      "2020-06-18 09:27:24,950 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-18 09:27:24,951 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-18 09:27:24,957 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-18 09:27:24,957 : INFO : EPOCH - 37 : training on 230224 raw words (204815 effective words) took 0.2s, 1364583 effective words/s\n",
      "2020-06-18 09:27:25,099 : INFO : worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-18 09:27:25,102 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-18 09:27:25,108 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-18 09:27:25,108 : INFO : EPOCH - 38 : training on 230224 raw words (204953 effective words) took 0.1s, 1374742 effective words/s\n",
      "2020-06-18 09:27:25,260 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-18 09:27:25,263 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-18 09:27:25,269 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-18 09:27:25,270 : INFO : EPOCH - 39 : training on 230224 raw words (205007 effective words) took 0.2s, 1301224 effective words/s\n",
      "2020-06-18 09:27:25,412 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-18 09:27:25,417 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-18 09:27:25,422 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-18 09:27:25,423 : INFO : EPOCH - 40 : training on 230224 raw words (204872 effective words) took 0.2s, 1363828 effective words/s\n",
      "2020-06-18 09:27:25,423 : INFO : training on a 9208960 raw words (8196627 effective words) took 6.2s, 1329982 effective words/s\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=20, min_count=2, epochs=40)\n",
    "model.build_vocab(train_corpus)\n",
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03474607 -0.15347145  0.32884297  0.10445941  0.13171554 -0.02941207\n",
      "  0.12577774  0.04781371 -0.09115019 -0.11344688  0.11629994  0.40579236\n",
      " -0.35446244 -0.03451527  0.2492189  -0.39030346  0.04649777 -0.31508806\n",
      "  0.19510715  0.03708351]\n"
     ]
    }
   ],
   "source": [
    "vector = model.infer_vector(['金融','行业','原油宝','期货'])\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-18 09:27:25,444 : INFO : precomputing L2-norms of doc weight vectors\n"
     ]
    }
   ],
   "source": [
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(train_corpus)):\n",
    "    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "\n",
    "    second_ranks.append(sims[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 975, 1: 14, 3: 2, 4: 2, 5: 2, 19: 1, 14: 1, 9: 1, 2: 1, 13: 1})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "counter = collections.Counter(ranks)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document (999): «新华社 耶路撒冷 月 日电 记者 尚昊 陈文仙 以色列 财政部 日 以色列 当天 亚洲 市场 首次 发行 债券 发行 总额 达 亿美元 财政部 本轮 债券 年期 利率 3.8% 旨在 以色列政府 减少 因新冠 疫情 财政赤字 疫情 发生 以色列 陆续 推出 多项 措施 应对 疫情 经济 冲击 月 日 以色列 推出 亿新 谢克尔 约合 228 亿美元 财政 救助 计划 规模 相当于 国内 生产总值 6% 月 日 以色列 面向 欧美 市场 发行 总额 达 亿美元 债券 月 日 以色列 中央银行 基准利率 0.25% 降至 0.1% 以色列 央行 本月 发布 数据 显示 受 疫情 影响 第一季度 以色列 经济 下滑 约 5% 预计 年 经济 萎缩 5.3% 以色列 卫生部 日晚 发布 新冠 疫情 数据 显示 疫情 发生 该国 累计 确诊 15834 例 累计 死亡 215 例 累计 治愈 8233 例»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d20,n5,w5,mc2,s0.001,t3):\n",
      "\n",
      "MOST (999, 0.9750403165817261): «新华社 耶路撒冷 月 日电 记者 尚昊 陈文仙 以色列 财政部 日 以色列 当天 亚洲 市场 首次 发行 债券 发行 总额 达 亿美元 财政部 本轮 债券 年期 利率 3.8% 旨在 以色列政府 减少 因新冠 疫情 财政赤字 疫情 发生 以色列 陆续 推出 多项 措施 应对 疫情 经济 冲击 月 日 以色列 推出 亿新 谢克尔 约合 228 亿美元 财政 救助 计划 规模 相当于 国内 生产总值 6% 月 日 以色列 面向 欧美 市场 发行 总额 达 亿美元 债券 月 日 以色列 中央银行 基准利率 0.25% 降至 0.1% 以色列 央行 本月 发布 数据 显示 受 疫情 影响 第一季度 以色列 经济 下滑 约 5% 预计 年 经济 萎缩 5.3% 以色列 卫生部 日晚 发布 新冠 疫情 数据 显示 疫情 发生 该国 累计 确诊 15834 例 累计 死亡 215 例 累计 治愈 8233 例»\n",
      "\n",
      "SECOND-MOST (238, 0.8533245921134949): «新华社 东京 月 日电 记者 刘春燕 日本参议院 月 日 批准 总额 达 25.69 万亿日元 美元 约合 107 日元 史 大规模 补充 预算案 补充 预算 用于 应对 新冠 疫情 众议院 已于 日 这一 补充 预算案 参议院 批准 意味着 财年 补充 预算案 国会 得以 财务 省 补充 预算 资金 发行 国债 筹措 日本国会 月 日 批准 总额 102.7 万亿日元 财年 年 月 财政 预算案 制定 预算案 时 日本政府 预测 财年 日本 经济 增长率 1.4% 专家 媒体 普遍认为 这一 预测 乐观 应对 新冠 疫情 经济 冲击 日本首相 安倍晋三 指示 内阁 成员 着手 编制 财年 补充 预算案 日本政府 月 日 总额 108 万亿日元 史 大规模 经济 刺激 计划 月 日 117 万亿日元 补充 预算案 随之 修改»\n",
      "\n",
      "MEDIAN (469, 0.37986138463020325): «月 日 北京 响应 解除 低 风险 地区 人员 进京 隔离 天 月 日 北京 高风险 地区 朝阳 风险 等级 降为 低 风险 帝都 区 皆 低 风险 月 日 零时 北京 收费 高速公路 恢复 收取 车辆通行 费 发酵 全国 两会 时间 全国性 解封 喜悦 一扫 昔日 疫情 阴霾 喜大普奔 奔走相告 当天 大型 OAT 平台 数据 显示 北京 出港 航班 搜索 量 增长 倍 租车 订单 恢复 去年 同比 周边 景区 访问量 暴涨 199% 环京 健康 游 人数 预计 超过 出京 机票 预定 量 暴增 倍 成都 重庆 上海 杭州 长沙 深圳 昆明 广州 西安 厦门 三亚 成 热门 目的地 囤 酒店 成风 抢购 高星 酒店 券数 大涨 650% 一场 报复性 消费 如期而至 遭遇 史 惨淡 开门红 走势 会否 报复性 消费 般 迎来 一波 保费 报复性 上涨 补缺 一季度 利空 出尽 一季度 市场 数据 InsuranceToday 第一重 利好 月 保费 回暖 环比 增 百分点 年初 一场 突如其来 疫情 保险业 措手不及 严峻 依赖 线下 渠道 寿险 行业 更是 苦不堪言 寄予厚望 开门红 黯然 落幕 年 第一个 月 中国 保险业 规模 保费 负增长 5% 第二个 月 规模 保费 负增长 升至 百分点 银保 渠道 更是 断崖 式 负增长 随处可见 比例 下滑 行至 月 这一 数字 改善 一季度 2.3% 原 保费 增幅 背后 月 单月 保费 大幅度 增长 同比 增长 5% 环比 增长 83% 月份 这一 数字 负增长 百分点 代表 市场走势 巨头 险企 更是如此 中国 人寿 中国 人保 601319 股 中国 平安 601318 中国 太保 新华 五大 上市 险企为 例 月 单月 保费 环比 增长 百分点 产险 业务 更是 百分点 增长 环比 大涨 120 百分点 寿险 保费 同比 降幅 收窄 0.3% 环比 增长 百分点 人身险 行业 月 保费 负增长 百分点 月 单月 负增长 百分点 InsuranceToday 第二重 利好 三大 险种 强势 回归 健康险 车险 农险 第一季度 保费 1.67 万亿 同比 增幅 2.3 百分点 看似 不高 背后 实则 第一个 月 月 负增长 回暖 涌现出 增长 强劲 险种 第一 增长极 非 健康险 莫属 表现 最为 靓丽 2641 健康险 保费 增量 达 469 增长 22% 月 当月 健康险 保费 破 千亿元 关口 达 1098 车险 保费 明显好转 负增长 转为 增长 月 财险 第一 险种 车险 负增长 百分点 月 单月 负增长 百分点 月 单月 车险 保费 723 同比 增长 百分点 第三 农业 强劲 复苏 第一季度 合计 保费 187 同比 增长 百分点 月 当月 劲收 亿 同比 增长 53% 整体而言 产险 公司 保费 表现 优于 寿险 公司 多线 拓展 渠道 典型 产险 公司 健康险 保费 超过 同比 增幅 超过 InsuranceToday 第三重 利好 监管 鼓励 一波 代理人 增员潮 来临 月 日银 保监会 参加 国务院新闻办 新闻 发布会 一句 代理人 队伍 发展 吸纳 劳动力 提供 就业 岗位 定调 寿险 步子 一份 内部 交流 数据 显示 第一季度 寿险 七家 合计 新增 人力 约 七八十万 联想 第一季度 GDP 数字 诉说 经济 环境 现实 失业 人口 注定 增加 冗于 家中 毕业 学生 旅游 销售 农民工 增员大年 上个世纪 九十年代 增员 窗口期 保费 压力 队伍 流失 双重 挤压 之下 增增 不休 各家 寿险 公司 关键词 一场 增员 大战 转型 叠加 疫情 新长征 路上 打响 展业 第一季度 寿险 公司 巨头 积累 足够 代理人 队伍 典型 中国 人寿 受益 代理人 队伍 扩量 影响 第一季度 销售 渠道 总 人力 超 大个险 月均 销售 人力 同比 增长 百分点 年 一曲 大规模 代理人 增员 到来 第二季度 保险公司 开门红 季度 月 单月 保费 反弹 举措 小试牛刀 InsuranceToday 第四重 利好 资本 市场 反弹 各国 争相 出台 刺激 计划 A股 1.3% 涨幅 结束 节前 交易日 跌跌 不休 资本 市场 保险市场 担心 全球 资本 市场 波动 加剧 险资 收益 稳定 加剧 保险公司 经营 波动 事实上 新冠 肺炎 疫情 海外 蔓延 各国 经济 面临 较大 冲击 国家 领导人 担忧 经济衰退 更好 应对 疫情 冲击 国家 纷纷 祭 货币政策 财政政策 经济 刺激 政策 提振 经济 如二 十国集团 领导人 携手 救 市 美股 二次 探底 猜测 节前 一周 海外 资本 市场 反弹 上涨 高开高 走 道琼斯 指数 节前 一举 越过 这波 反弹 高点 国内 股市 上涨 原因 事实上 一点 环境 息息相关 第一季度 业绩 出台 可谓 利空 出尽 上市公司 公布 年报 季报 阶段 皆 受 疫情 影响 公司 季报 业绩 难堪 事实上 业绩 想象 差 资本 市场 信心 利空 相继 迭出 二季度 资本 市场 行情 能比 疫情 之初 差 InsuranceToday 第五重 利好 亿 承保 利润 加持 同比 增幅 2000% 份 流传 圈 交流 数据 显示 年 第一季度 财险 公司 承保 利润 年 同期 数字 负 产险 第一 险种 车险 承保 利润 高达 增幅 百分之 一万五 承保 利润 百分点 这是 车险 业绩 同期 亏损 大户 健康险 承保 利润 减少 依旧 亏损 大户 信用 承保 盈利 去年同期 亏损 亿 爆 雷区 保证 亏损 达 亿 去年 盈利 亿 一点 月 日 国新办 发布会 给出 解释 第一季度 保证 赔付 支出 161 同比 增长 约 原因 企业 个人收入 减少 还款 能力 下降 违约 率 增加 承保 利润 提高 产险 第一季度 利润 表现 优于 寿险 公司 原因 InsuranceToday 第六 重 利好 佣金 费用 下滑 弹药 积累 这是 有意思 现象 第一季度 保费 2.3 百分点 增长 保险公司 各项 开支 受制于 业务 发展 节奏 负增长 一正一负 皆 费用 空间 或者说 利润 空间 第一季度 保险公司 手续费 佣金 支出 共计 1634 负增长 110 业务 管理费 支出 共计 1246 同比 节省 两项 合计 132 关键 指标 第一季度 赔付 支出 同比 减少 下降 百分点 较大 现金流 支出 人身险 公司 退保 金 同比 下降 55% 上千亿 规模 这皆为 三季度 冲刺 提供 弹药 支持 InsuranceToday 第七 重 利好 利空 出尽下 保费 饥渴 商业 经营 角度 第一季度 保险行业 利空 非 保费 投资 整体 利润 负 增长幅度 超过 两位数 1002 第一季度 利润 去年同期 足足 少 170 亿 规模 保险公司 最是 苦楚 利润 负增长 利润 其间 重灾区 规模 寿险 公司 利润 负增长 百分点 保费 增长 百分点 保户 投资 款 独立 账户 年 新增 交费 下降 约 百分点 可怕 事情 保费 费用 保费 投资收益 逻辑 解释 年 第一季度 保险业 净利润 下滑 预示 后期 保费 渴求 逻辑 业绩 背后 注定 三季度 凶猛 公司 下调 市场 经营 目标 情况 月 月 时间 月 InsuranceToday 第八 重 利好 后记 全国性 复 工潮 保费 需求 大型 公共卫生 危机 背后 激发 社会 风险意识 群众 意识 提升 激发 需求 健康险 崛起 证明 一点 全国 宏观经济 角度 月 中旬 全国 规模 企业 平均 开工率 99% 全国性 复 工潮 到来 常态 防疫 口径 百姓生活 回归 平常 解封 疫情 一点 新冠 肺炎 疫情 改变 保险行业 向上 发展趋势 还会 加速 行业 爆发 变革 进程 经受 疫情 洗礼 脱节 数字化 或者说 科技 能力 现状 改善 加速 走向 线上 线下 融合 意识 提升 复工 大潮 来临 数字 加速 预示 更好 未来 本文 首发 微信 公众 号 今日 保 文章内容 属 作者 个人观点 代表 和讯网 立场 投资者 操作 风险 请 自担 责任编辑 董 云龙»\n",
      "\n",
      "LEAST (591, -0.14880341291427612): «高频 脉冲 水线 轻松 祛除 牙齿 齿间 食物残渣 崂山 可乐 平常 可乐 相比 气 足 入口 刺激 细品 一股 药草 香味 王老吉 盐汽水 精选 人参 石榴 果汁 夏天 吃 烧烤 喝 健康 盐汽水 线下 商超 听 商家 亏本 冲量 抓紧时间 抢购 采用 老料 压制 熟 茶 黄金 品饮期 产量 市场 存量 有限 自然 陈化 汤色 红艳 纯正 醇和 甜净 茶底 红褐 油润 西安 冰峰 汽水 陕西 特产 罐 夏日 冰爽 橙 味十足 年 味道 记忆 味道 陕西 特产 童年 回忆 佳琦 助理 推荐 全家福 瓶装 口味 惊喜 添加 鲜果汁 蔗糖 负担 新鲜 口感 赠 运费 险 一款 循环 扇 落地扇 循环 扇 清洁 室内空气 直 吹 满天星 椰子 鞋 一款 夜间 发光 鞋 夜间 行走 新款 首发 时尚 潮流 穿着 透气 舒适 防滑 耐磨 底 潮出 范 多款 多色 送 运费 险 柔软 双面绒 可机 洗 双面 四季 可用 瞬间 转换 座 一体 久坐 舒适 减少 疲累 设计 注重 细节 品质 适配 椅型 赶紧 抢 赠 运费 险 配料表 生 牛乳 含量 ≥ 指 生 牛乳 重量 ≥ 冰淇淋 重量 杯生 牛乳 一根 悠致 食用 健康 孩子 吃 放心 爆款 返场 余文乐 同款 t 恤 14.9 高品质 纯棉 面料 潮流 百搭 道工序 磨毛 质量 不输 专柜 大牌 经典 圆领 设计 简洁 大方 采用 环保 活性 印染 工艺 持久 耐穿 水洗 褪色 穿着 放心 赠 运费 险 验货 精选 优质 桑蚕丝 柔软 舒适 亲肤 透气 起球 退色 立体 时尚 版型 简约 大方 做工 精细 走线 工整 啄木鸟 品牌 授权 店 支持 专柜 扫码 验货 值得 信赖 包邮赠 运费 险 高 评分 多款 选 带不带 图案 价 神价 速度 百分百 纯棉 千人 验货 质量 爆 超级 品质 长绒棉 吸汗 透气 简约 百搭 起球 掉色 抓紧 老公 囤货 崂山 白花蛇 草水 奇特 口味 口感 清爽 弱碱 无糖 称为 水中 贵族 映趣 波米 专卖店 理发器 静音 充电式 宝宝 剃发 儿童 剃头 幼儿 电推 剪头发 家用 超 陶瓷 刀头 静音 防水 设计 送 理发 工具 快速 充电 理发 伤 头皮 夹发 不卡发 宝宝 理发 妈妈 放心 抖音 网红 爆款 通用 支撑架 折叠式 升降 自由 升降 总有 角度 适合 全新 升级 支架 充电 宝 硅胶 防滑 面板 均匀 受力 打滑 不伤 爱机 京东 同款 只券后 8.9 元起 铅玻璃 健康 环保 耐冷 耐热 易清洗 琥珀色 系 简约 大方 双耳 设计 隔热 防烫 端 可用 来装 沙拉 水果 点心 分 分钟 提升 仪式 感好 赠 运费 险 FZP 天猫 正品 夏季 新款 男 T恤 属实 透气 时尚 百搭 褪色 心 款式 供选择 休闲 潮流 赠 运费 险 关注 同花顺 财经 ths518 获取 机会»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Document (12): «AFP2020 MANDELNGAN 白宫 顾问 指责 中国 美国 交付 合格 新冠 病毒检测 试剂盒 美国 国家情报局 统计 中称 冠状病毒 人为 制造 基因 编辑 一事 情报界 已达成 科学 共识 美国 科学家 研究 病毒 传播 感染 动物 接触 中国 武汉市 实验室 违反 特朗普 此前 美方 研究 新冠 病毒 COVID 中国 武汉市 实验室 里 打算 查明 病毒 起源 特朗普 威胁 中国 查出 北京 新冠 疫情 爆发 承担 后果 此前 中国外交部 发布 消息 称 新冠 肺炎 疫情 发生 中国 公开 透明 负责 态度 发布 疫情 信息»\n",
      "\n",
      "Similar Document (359, 0.8740598559379578): «原 标题 锐 参考 中国 外交 天团 本周 连发 数十 问 美国 请 回答 新冠 肺炎 疫情 美国 突破 上限 美国 政客 频频 秀 下限 美国 国务卿 蓬佩奥 白宫 国家 贸易 制造业 政策 办公室 主任 纳 瓦罗 代表 近期 炮制 种种 匪夷所思 阴谋论 试图 栽赃 中国 资料 图片 美国 国务卿 蓬佩奥 新华社 面对 美国 频频 泼来 脏水 中国外交部 发言人 发起 反击 每日 例行 记者会 澄清事实 海外 社交 媒体 平台 发声 据小锐 统计 本周 外交部 发言人 华春莹 推特 发问 耿爽 记者会 左右开弓 外交部 副 部长 乐 玉成 直面 美媒 场合 语境 议题 设置 之下 美国 疫情 数据 病毒 源头 调查 话题 中国 外交官 美方 连续 发问 超过 次 种种 追问 美方 欠 世界 答案 华春莹 两怼 蓬佩奥 问纳 瓦罗 不请 美国 专家 查明 新冠 病毒 最早 美国 何地 月 日 中午 外交部 发言人 华春莹 推特上 发出 此问 美国 答案 世界 有权 知情 推文 所配 图片 华春莹 反问 对象 美国 国务卿 蓬佩奥 近日 推特上 煽风点火 声称 中国 展开 调查 华春莹 反击 蓬佩奥 推特 截图 本周 华春莹 推特上 第二次 点名 回击 蓬佩奥 月 日 蓬佩奥 福克斯 新闻频道 指责 中国 隐瞒 疫情 论调 华春莹发 推 反问 散布 假消息 共和党 参议院 全国 委员会 灰色 地带 美国 一家 独立 新闻 网站 披露 华春莹 一问 指向 此前 美媒 爆出 反华 备忘录 这份 长达 页 备忘录 美国共和党 参议院 全国 委员会 炮制 用以 指导 竞选 机构 套路 攻击 中国 怼 完蓬佩奥 当晚 华春莹 推特 对纳 瓦罗 连发 三问 这是 著名 罗恩 瓦拉 说 假 检测 试剂盒 囤积 防护 装备 此三问 纳 瓦罗 近日 谬论 指责 中国 美国 出口 假冒伪劣 检测 试剂 疫情 牟取暴利 华春莹 反击 纳 瓦罗 推特 截图 反击 推 文中 华春莹 选择 数字 说话 指出 中方 美方 机构 抗疫 物资 出口 合作 出口 产品质量 符合 美国 标准 月 日至 月 日 中国 美国 提供 约 亿个 口罩 亿副 手套 5800 多台 呼吸机 列 数据 摆事实 讲道理 回击 质疑 反对 抹黑 主动 发问 小锐 粗略 统计 一周 华春莹 发出 条 推特 涉美 话题 占 半数以上 可谓 火力 全开 耿爽 抛出 连珠炮 问 请 美国 回答 如果说 华春莹 推特 展开 线上 对抗 置身 外交部 蓝厅 耿爽 线下 直面 外媒 记者 偏见 质疑 耿爽 美国 发问 日 记者会 当天 耿爽 提及 美国共和党 参议院 全国 委员会 炮制 反华 备忘录 应询 指出 美方 口口声声 说 中方 散布 虚假 信息 报道 属实 不禁 问 散布 虚假 信息 同日 借力 打力 以三连 问 美媒 报道 质疑 包括 疫情 最早 美国 美国政府 隐瞒 急 国家 国际 组织 甩锅 希望 美国政府 回应 本国 国际 社会 关切 请 世卫 组织 协助 调查 月 日 耿爽 主持 外交部 例行 记者会 外交部 网站 事实上 从线 线下 互动 海外 社交 媒体 例行 记者会 默契 十足 发问 耿爽 华春莹 两位 外交部 发言人 互文 联动 日华春莹 反问 推 文中 提到 罗恩 瓦拉 想必 问 这人 答案 月 日 耿爽 例行 记者 表态 当天 耿爽 指出 去年 多家 美媒 披露 纳 瓦罗 多部 书作 引用 专业人士 罗恩 瓦拉 言论 批评 攻击 中国 纳 瓦罗 公开 承认 虚构 这名 反华 专家 令 国际 社会 大跌眼镜 感到 震惊 一场 采访 外交部 副 部长 抛出 问号 隔空 发问 坐下 面对面 交锋 日 外交部 副 部长 乐 玉成 接受 美国全国广播公司 NBC 资深 记者 马静 JanisMackeyFrayer 专访 这场 采访 乐 玉成 次以 反问 回答 美国 记者 提问 月 日 外交部 副 部长 乐 玉成 接受 美国全国广播公司 电视 专访 外交部 网站 问及 中国 支持 展开 病毒 源头 国际 调查 时 乐 玉成 以四连 问 给出 回答 国际 调查 调查 中国 证据 中国 不去 国家 开展调查 反思 科学 角度看 国家 确诊 病例 死亡 病例 扩散 病例 说 美国 指责 中国 疫情 初期 乐 玉成 列 数据 指出 月 日 武汉 封城 时 美国 病例 月 日 美国 国家 紧急状态 时 确诊 病例 多例 乐 玉成 连续 发问 这能 中国 慢 时间 事实上 中国 外交官 持续 发问 白宫 疫情 简报 美国 媒体 互联网 引发 讨论 社交 媒体 越来越 外国 网友 美国政府 做法 问题所在 点击 专题 聚焦 新型 冠状病毒 肺炎 疫情 全球 国 爆发 新冠 肺炎 疫情 美国 成 疫情 震中 责任编辑 张申»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pick a random document from the corpus and infer a vector from the model\n",
    "import random\n",
    "doc_id = random.randint(0, len(train_corpus) - 1)\n",
    "\n",
    "# Compare and print the second-most-similar document\n",
    "print('Train Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "sim_id = second_ranks[doc_id]\n",
    "print('Similar Document {}: «{}»\\n'.format(sim_id, ' '.join(train_corpus[sim_id[0]].words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import codecs\n",
    "import math\n",
    "BITS = 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simHash(object):\n",
    "# 初始化，遍历文档（已分词），得到词汇表，并进行32位(能表示2^32种情况，完全足够)\\\n",
    "#hash编码和对应的idf值，将idf值作为其权重进行运算，分别存入两个字典(dict)\n",
    "    def __init__(self, documents):\n",
    "        f = documents\n",
    "        dictHash = dict()\n",
    "        dictWeight = dict()\n",
    "        i = 0#hash编码\n",
    "        lines = 0#记录文本数量,以计算idf值\n",
    "        #遍历文本，进行hash编码和统计df词频(在多少篇文章出现过，而不是总词频，\\\n",
    "        #比如某个词在一个文本中出现三次也只算一次)\n",
    "        for line in f:\n",
    "            lines += 1\n",
    "            temp = set(str(line).strip().split())#避免重复统计词频\n",
    "            for item in temp:\n",
    "                if item not in stopwords:\n",
    "                    if item not in dictWeight:\n",
    "                        dictWeight[item] = 1\n",
    "                        dictHash[item] = i\n",
    "                        i += 1\n",
    "                    else:\n",
    "                        dictWeight[item] += 1\n",
    "        del i\n",
    "        #hash编码转为array形式的二进制，方便计算\n",
    "        for item in dictHash:\n",
    "            L = list(bin(dictHash[item]))[2:]\n",
    "            intL = [int(x) for x in L]\n",
    "            for i in range(len(intL)):\n",
    "                if intL[i] == 0:\n",
    "                    intL[i] = -1\n",
    "            intL = (BITS - len(intL))*[-1]+intL\n",
    "            dictHash[item] = np.array(intL)\n",
    "        #根据词频计算idf值\n",
    "        for item in dictWeight:\n",
    "            dictWeight[item] = math.log(lines/dictWeight[item])\n",
    "\n",
    "        self.dictHash = dictHash\n",
    "        self.dictWeight = dictWeight\n",
    "        \n",
    "    #根据词的hash对句子进行hash编码\n",
    "    def senHash(self, sen):\n",
    "        senHashCode = np.zeros(BITS)\n",
    "        temp = sen.strip().split()\n",
    "        for item in temp:\n",
    "            senHashCode += self.dictHash[item]*self.dictWeight[item]\n",
    "        for i in range(BITS):\n",
    "            if senHashCode[i] > 0:\n",
    "                senHashCode[i] = 1\n",
    "            else:\n",
    "                senHashCode[i] = 0\n",
    "        return senHashCode\n",
    "\n",
    "    #获取两个句子的Hamming distance，dis越小说明相似度越高\n",
    "    def sen2senDis(self, sen1, sen2):\n",
    "        temp1 = self.senHash(sen1)\n",
    "        temp2 = self.senHash(sen2)\n",
    "        Hamming = 0\n",
    "        for i in range(BITS):\n",
    "            if temp1[i] != temp2[i]:\n",
    "                Hamming += 1\n",
    "        return Hamming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "simhash = simHash(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1  1  1\n",
      " -1 -1 -1 -1 -1 -1  1]\n",
      "2.7230880784667506\n"
     ]
    }
   ],
   "source": [
    "print(simhash.dictHash['金融'])\n",
    "print(simhash.dictWeight['金融'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simhash.sen2senDis(documents[885], documents[684])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 304\n"
     ]
    }
   ],
   "source": [
    "minSimHash = 10000\n",
    "minSimHashIndex = -1\n",
    "for i in range(1, n):\n",
    "    temp = simhash.sen2senDis(documents[0], documents[i])\n",
    "    if temp < minSimHash:\n",
    "        minSimHash = temp\n",
    "        minSimHashIndex = i\n",
    "        \n",
    "print(minSimHash, minSimHashIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "新华社 哥本哈根 月 日电 记者 林晶 世界卫生组织 欧洲 区域 办事处 主任 克卢格 月 日 哥本哈根 视频 例会 时 呼吁 区域 各国 特别 社区 传播 控制 国家 应 确保 医疗卫生 系统 双轨制 抗击 新冠 疫情 保证 常规 医疗卫生 服务 运转 克卢格 说 欧洲地区 新冠 疫情 严峻 一周 累计 确诊 病例 15% 累计 确诊 病例 达 1408266 例 同期 死亡 病例 18% 累计 死亡 人数 达 129344 欧洲地区 累计 确诊 死亡 病例 占 世界 相关 病例 数 46% 63% 克卢格 呼吁 欧洲各国 政府 卫生机构 寻求 办法 控制 新冠 病毒 社区 传播 前提 快速 恢复 常规 医疗卫生 服务 特别 指出 形势 保证 儿童 接种 麻疹 常规 疫苗 重要性 克卢格 说 新冠 疫情 短时期 消失 波 第三 波 疫情 认知 动员 社会 理解 协作 双轨制 医疗卫生 系统 保证 应对 新冠 疫情 反复 时 灵活性 弹性\n"
     ]
    }
   ],
   "source": [
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "月 日 统计数据 荷兰 累计 感染 病例 38802 例 4711 死亡 医务人员 补充 道 新冠 病毒感染 死亡 病例 新增 例 升至 4795 例 超过 1.07 医院 接受 治疗 世卫 组织 月 日 COVID 疫情 定性 流行 世卫 组织 最新 数据 全球 累计 确诊 万多例 新冠 病毒感染 病例 累计 死亡 20.8 万多例\n"
     ]
    }
   ],
   "source": [
    "print(documents[304])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[304, 503, 529, 608, 698, 699, 876, 886, 911, 983, 986]\n",
      "[1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2]\n",
      "1.0834150314331055\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "simIndex = []\n",
    "simNum = []\n",
    "# minSimHash = 10000\n",
    "# minSimHashIndex = -1\n",
    "for i in range(1, n):\n",
    "    temp = simhash.sen2senDis(documents[0], documents[i])\n",
    "    if temp < 3:\n",
    "        simIndex.append(i)\n",
    "        simNum.append(temp)\n",
    "#     if temp < minSimHash:\n",
    "#         minSimHash = temp\n",
    "#         minSimHashIndex = i\n",
    "        \n",
    "end_time = time.time()\n",
    "print(simIndex)\n",
    "print(simNum)\n",
    "print(end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "格隆汇 月 日 丨 唐德 影视 300426 SZ 公布 年 月 日 公司 浙江 东阳 昊美 影业 有限公司 昊美 影业 公司 昊美 影业 合称 签订 战略 合作 协议 协议 约定 战略 合作 建立 长期 战略 合作伙伴 关系 提供 资源 项目 影视 领域 多层次 全方位 深度 合作 包括 限于 影视 项目 投资 开发 共享 优质 资源 拓展 发行 渠道 联动 宣传 推广 协议 签署 生效 互为 优先选择 合作伙伴 应 建立 长期有效 沟通 机制 合作 顺利进行 创造 有利条件 确保 实施 全方位 合作 配合 优化 合作 模式 市场化 运作 强强 联手 确保 合作 项目 既有 社会 影响力 市场需求 相结合 确保 社会效益 投资 效益 互利 共赢 影视 项目 合作 确立 影视 项目 战略 合作 关系 投资 开发 影视 项目 包括 限于 电影 电视剧 网络 剧 网络 电影 电视栏目 现有 未来 影视作品 形式 相关 衍生品 共享 投资收益 共担 投资 风险 共享 发行 渠道 项目 收益 最大化 提升 核心 竞争力 影视 项目 合作 权利义务 签署 项目 联合 投资 摄制 合同 约定 宣传 推广 合作 长期 战略 合作伙伴 充分利用 旗下 网站 平台 资源 合作项目 推广 工作 合作项目 联动 宣传 推广 协议 约定 合作 期限 协议 签署 生效 计算 合作 期满 一个月 协商 续签 协议 合作 全球 合作 公司 影视 产业链 优化 上下游 产业 拓展 影响 有助于 公司 整合 资源优势 进一步 完善 影视 产业链 战略 布局 公司 影视 相关 产业 运营 发展 起到 推动 作用 进一步 增强 公司 持续 发展 能力 核心 竞争力 责任编辑 HN666\n"
     ]
    }
   ],
   "source": [
    "print(documents[876])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#minhash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasketch import MinHash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'昨晚 美股 率先 突破 站上 日 均线 创新 高 东京 日经指数 涨幅 超 2% 创新 高 A股 大板 指 高开高 走 上证 一举 拿下 久攻不下 2850 点 成交量 暴增 银行 强 银行 板块 昨天 起到 应有 护盘 作用 轮 证券 板块 启动 大涨 超 2% 上证指数 支撑 爆量 上涨 资金 光速 进场 资金 板块 半导体 板块 板块 指数 暴涨 7% 创 历史 单日 涨幅 板块 权重股 行业龙头 涨停 如兆易 创新 通富 微电 长电 科技 沪 硅 产业 板块 效应 板块 效应 吸引 资金 参与 交易 好事 板块 技术 面 短期 均线 均线 粘合 短期 平均 成本 重叠 爆量 暴涨 站 上半年 线 近半年 平均 持仓 成本 获利 人类 趋利避害 特性 越 赚钱 越 持股 买入 越 亏损 越 卖出 恐慌性 吸引 资金 参与 半导体 指数 无线耳机 板块 板块 指数 涨幅 超 5% 佳禾 智能 天 板 市场 龙头股 华胜天 成 突破 年 高点 涨停板 亿 资金 抢筹 封板 突破 趋势 安洁 科技 PE30 倍 估值 不高 流通 市值 亿 年线 启动 早盘 开盘 分钟 涨停 速度 之快 前所未有 资金 心情 急切 可见一斑 华胜天 成 科技股 走势 板块 食品饮料 农林牧渔 人造肉 医药 板块 究其原因 逻辑 是因为 农林牧渔 食品饮料 人造肉 医药 消费 板块 偏 防御 指数 一跌 板块 必涨 这招 屡试不爽 A股 市场 经验 规律性 质疑 当作 公理 来记 当作 记住 预计 指数 五一 后先 延续 上涨 态势 科技股 领涨 交易日 指数 遇压 调整 指数 调整 防御性 板块 食品饮料 农林牧渔 人造肉 医药 复制 前期 行情 下图 农林牧渔 板块 指数 上证指数 叠加 图 上半 农林牧渔 板块 指数 上证指数 走势 农林牧渔 上证指数 叠加 图 透露 观察 市场 情绪 指标 规律 前天 早盘 A股 跌停 数量 上次 跌停 数量 好巧 巧合 市场 恐慌 情绪 聪明 资金 一看 熟悉 情况 情绪 低谷 来临 拐点 到来 久违 百股 涨停 情绪 时间 周期 情绪 周期 高点 低点 天 时间 低点 高点 天 时间 时间 做 防御性 板块 情绪 低谷 指标 市场 情绪 退潮 期时 市场 最先 跌停 数量 增多 连续 跌停 股 数量 增多 恐慌性 顶 跌停 数量 只股 跌停 跌停 数量 减少 涨停 数量 增多 高潮 指标 充分利用 市场 情绪 拐点 情绪 低点 时 布局 情绪 高潮 规避 转向 防御性 板块'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data0 = documents[0].split()\n",
    "data1 = documents[1].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "m0, m1 = MinHash(), MinHash()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data0:\n",
    "    m0.update(d.encode('utf-8'))\n",
    "for d in data1:\n",
    "    m1.update(d.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Jaccard for data0 and data1 is 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Estimated Jaccard for data0 and data1 is\", m0.jaccard(m1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Jaccard for data1 and data2 is 0.006872852233676976\n"
     ]
    }
   ],
   "source": [
    "s0 = set(data0)\n",
    "s1 = set(data1)\n",
    "actual_jaccard = float(len(s0.intersection(s1)))/float(len(s0.union(s1)))\n",
    "print(\"Actual Jaccard for data1 and data2 is\", actual_jaccard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minHashSim(d0, d1):\n",
    "    data0 = d0.split()\n",
    "    data1 = d1.split()\n",
    "    m0, m1 = MinHash(), MinHash()\n",
    "    for d in data0:\n",
    "        m0.update(d.encode('utf-8'))\n",
    "    for d in data1:\n",
    "        m1.update(d.encode('utf-8'))\n",
    "#     print(\"Estimated Jaccard for data0 and data1 is\", m0.jaccard(m1))\n",
    "#     s0 = set(data0)\n",
    "#     s1 = set(data1)\n",
    "#     actual_jaccard = float(len(s0.intersection(s1)))/float(len(s0.union(s1)))\n",
    "#     print(\"Actual Jaccard for data1 and data2 is\", actual_jaccard)\n",
    "    return m0.jaccard(m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Jaccard for data0 and data1 is 0.015625\n",
      "Actual Jaccard for data1 and data2 is 0.021377672209026127\n"
     ]
    }
   ],
   "source": [
    "minHashSim(documents[0], documents[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total run time: 6.4895339012146\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "maxJaccard = 0\n",
    "maxJaccardIndex = 0\n",
    "for i in range(1, n):\n",
    "    temp = minHashSim(documents[0], documents[i])\n",
    "    if temp > maxJaccard:\n",
    "        maxJaccardIndex = i\n",
    "        maxJaccard = temp\n",
    "end_time = time.time()\n",
    "print(\"total run time:\", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15625 509\n",
      "新华社 基辅 月 日电 记者 李东旭 乌克兰 卫生部长 马克西 玛 斯捷潘 诺夫 日 说 乌克兰 单日 新增 新冠 确诊 病例 再创新高 达 540 例 累计 确诊 病例 10406 例 斯捷潘 诺夫 当天 新闻 发布会 儿童 累计 确诊 病例 717 例 医护人员 累计 确诊 病例 2663 例 乌克兰 累计 治愈 病例 1238 例 累计 死亡 病例 261 例 乌通社 援引 乌 经济 发展 贸易部 消息 说 乌克兰 日 恢复 872 农产品 市场 运营 斯捷潘 诺夫 说 乌克兰 农产品 农产品 市场 销售 开放 农产品 市场 经济 食品 价格 稳定 乌政府 制定 应对 方案 开放 农产品 市场 力争 疫情 传播 风险 降到 最低 月 日 乌政府 全国 隔离 措施 日 延长 月 日 斯捷潘 诺夫 说 隔离 措施 到期 小企业 解禁 包括 小型 商店 美发店 日用百货 店\n"
     ]
    }
   ],
   "source": [
    "print(maxJaccard, maxJaccardIndex)\n",
    "print(documents[maxJaccardIndex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pDL] *",
   "language": "python",
   "name": "conda-env-pDL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
