{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stopwords(stopwords_path):\n",
    "    with open(stopwords_path, 'r', encoding='utf-8') as f:\n",
    "        return [line.strip() for line in f]\n",
    "    \n",
    "def preprocess_data(corpus_path, stopwords):\n",
    "    corpus = []\n",
    "    with open(corpus_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            corpus.append(' '.join([word for word in jieba.lcut(line.strip()) if word not in stopwords]))\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "2020-06-10 11:37:09,440 : DEBUG : Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/m3/4yh806w92fdgcn0bk16ql7nw0000gn/T/jieba.cache\n",
      "2020-06-10 11:37:09,443 : DEBUG : Loading model from cache /var/folders/m3/4yh806w92fdgcn0bk16ql7nw0000gn/T/jieba.cache\n",
      "Loading model cost 0.616 seconds.\n",
      "2020-06-10 11:37:10,058 : DEBUG : Loading model cost 0.616 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "2020-06-10 11:37:10,059 : DEBUG : Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "n = 50\n",
    "stopwords_path = \"../data/stop_words.txt\"\n",
    "documents_path = \"../data/documents_first_\" + str(n) + \".txt\"\n",
    "stopwords = load_stopwords(stopwords_path)\n",
    "documents = preprocess_data(documents_path, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'董明珠 惊人 之语 炮轰 美的 怒斥 国产车 炮火 引向 大众 这下 本来 习惯于 看热闹 吃 瓜 群众 答应 踩 同行 骂 竞争对手 意见 敢动 老子 一亩 三分 不行 014 月 日 晚间 格力电器 董事长 兼 总裁 董明珠 接受 采访 时 赞同 黄奇帆 取消 住房 公积金 说 格力电器 3700 套 房子 员工 入住 未来 格力 员工 发一 套房 公积金 听听 话 饱汉不知饿汉饥 专程来 炫富 经济 下行 大众 钱包 吃紧 格力 本事 每名 员工 分 房子 本事 格力 分房 取消 全国 公积金 站长 想 问 一句 董 小姐 蠢 坏 确实 格力 优秀 年 营业 收入 1981.53 拥有 万名 员工 格力 万名 员工 发一 套房 站长 说 信 格力 真 员工 分房 中国 企业 众多 能发 房子 企业 凤毛麟角 特别 受 疫情 影响 众多 行业 暴击 企业 濒临 倒闭 活着 不错 每人 发 套房 格力 员工 公积金 取消 公积金 格力 确实 省下 一大笔钱 我国 亿人 贫富差距 取消 公积金 势必会 影响 人群 利益 也许 董明珠 换种 表述 特定 福利 企业 取消 缴纳 公积金 企业 节约 成本 用于 研发 创新 公积金 整体 实施 影响 我国 当初 建立 住房 公积金 制度 新加坡 学习 希望 强制性 缴纳 办法 集合 政府 企业 职工 三方 力量 解决 民众 购房 中国 最先 实行 公积金 政策 上海 全国 房地产 市场 发展 实行 公房 分配制度 家庭 人均 住房面积 七八 平方米 住 拥挤 居住 环境 急需 改善 公积金 强制 缴存 看似 个人收入 减少 长期 并非如此 民企 公积金 缴纳 比例 5% 12% 薪资 基数 缴纳 比例 6% 公司 6% 12% 公积金 存缴 数额 50006% 别看 元不多 长此以往 可不是 小数 缴纳 时间 公积金 买房 提取 大众 福利 特别 事业单位 公务员 群体 公积金 缴纳 金额 高 一般来说 公务员 月 公积金 扣除 比例 工资 12% 公积金 政策 国家 补贴 数额 公务员 一个月 公积金 工资 24% 民企 两倍 账面 工资 特别 高 公务员 群体 公积金 住 建部 人民银行 总行 统计 显示 年 全国 住房 公积金 缴存 总额 14549.46 上年 增长 12.29% 缴存 人数 机关 事业单位 工作人员 国企 职工 缴存 住房 公积金 比例 占 年 缴存 总额 60.16% 占 高 年 城镇 私 民 营 企业 城镇 企业 缴纳 住房 公积金 占 总额 19.5% 公积金 公务员 群体 至关重要 普通人 公积金 房价 高昂 公积金 居民 低息 房贷 唯一 渠道 全国 住房 公积金 年 年度报告 显示 年末 累计 发放 住房贷款 3334.82 万笔 85821.32 人员 置业 首 套房 贷款 年 公积金 利率 3.25% 二套 3.75% 远 商贷 首 套房 贷款 平均 利率 5.5% 假设 贷款 年 期限 公积金 贷款 商业贷款 节省 利息 约 笔 不小 资金 取消 公积金 将会 损害 大部分 利益 举个 例子 刘 缴纳 公积金 比例 12% 月 缴存 652 年 3.25% 利率 贷款 购买 一套 住房 每月 还款 可用 公积金 冲抵 979 每月 还款 多元 公积金 减轻 购房 压力 用处 很大 缴纳 住房 公积金 好处 少 缴纳 个人所得税 计算 个税 减去 住房 公积金 数额 不论是 单位 缴纳 缴纳 实惠 很大 公积金 提取 不断扩大 公积金 效率 提升 买房 装修 可用 租房 大病 提取 当今 疫情 部门 发出通知 新冠 肺炎 患者 提取 住房 公积金 用于 医疗 支出 买房 账户 里 公积金 退休 取出 生活 改善 说 公积金 用处 只能 租房 买房 公积金 表面 事 关乎 国家 大部 利益 开发商 公积金 制度 降低 买房 门槛 买房 人会 有利于 房企 发展 地方 政府 开发商 有钱 买 缴纳 土地 出让金 有利于 地方 财政 说 专家 说 取消 公积金 可取 稍微 动动脑 子 甄别 利弊 类 声音 就行 跑 偏 免责 声明 本文 腾讯 新闻 客户端 媒体 代表 腾讯网 观点 立场'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [[word for word in document.split()] for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "texts = [[token for token in text if frequency[token] > 2] for text in texts]\n",
    "# pprint(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-10 11:37:12,223 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-06-10 11:37:12,231 : INFO : built Dictionary(1236 unique tokens: ['15%', '一周', '世界', '主任', '人数']...) from 50 documents (total 10487 corpus positions)\n",
      "2020-06-10 11:37:12,232 : INFO : saving Dictionary object under ../data/first_50_doc.dict, separately None\n",
      "2020-06-10 11:37:12,234 : INFO : saved ../data/first_50_doc.dict\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(1236 unique tokens: ['15%', '一周', '世界', '主任', '人数']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.save('../data/first_' + str(n) + '_doc.dict')\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-10 11:37:12,244 : INFO : storing corpus in Matrix Market format to ../data/first_50_doc.mm\n",
      "2020-06-10 11:37:12,246 : INFO : saving sparse matrix to ../data/first_50_doc.mm\n",
      "2020-06-10 11:37:12,247 : INFO : PROGRESS: saving document #0\n",
      "2020-06-10 11:37:12,254 : INFO : saved 50x1236 matrix, density=6.827% (4219/61800)\n",
      "2020-06-10 11:37:12,256 : INFO : saving MmCorpus index to ../data/first_50_doc.mm.index\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize('../data/first_' + str(n) + '_doc.mm', corpus)\n",
    "# pprint(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-10 11:37:12,262 : INFO : collecting document frequencies\n",
      "2020-06-10 11:37:12,263 : INFO : PROGRESS: processing document #0\n",
      "2020-06-10 11:37:12,264 : INFO : calculating IDF weights for 50 documents and 1236 features (4219 matrix non-zeros)\n"
     ]
    }
   ],
   "source": [
    "from gensim import models, similarities\n",
    "tf_idf = models.TfidfModel(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1),\n",
      "  (1, 1),\n",
      "  (2, 1),\n",
      "  (3, 1),\n",
      "  (4, 1),\n",
      "  (5, 2),\n",
      "  (6, 1),\n",
      "  (7, 3),\n",
      "  (8, 4),\n",
      "  (9, 1),\n",
      "  (10, 2),\n",
      "  (11, 4),\n",
      "  (12, 1),\n",
      "  (13, 1),\n",
      "  (14, 1),\n",
      "  (15, 2),\n",
      "  (16, 1),\n",
      "  (17, 3),\n",
      "  (18, 1),\n",
      "  (19, 1),\n",
      "  (20, 1),\n",
      "  (21, 1),\n",
      "  (22, 1),\n",
      "  (23, 1),\n",
      "  (24, 2),\n",
      "  (25, 1),\n",
      "  (26, 1),\n",
      "  (27, 5),\n",
      "  (28, 1),\n",
      "  (29, 1),\n",
      "  (30, 1),\n",
      "  (31, 2),\n",
      "  (32, 2),\n",
      "  (33, 2),\n",
      "  (34, 1),\n",
      "  (35, 3),\n",
      "  (36, 2),\n",
      "  (37, 1),\n",
      "  (38, 5),\n",
      "  (39, 5),\n",
      "  (40, 1),\n",
      "  (41, 1),\n",
      "  (42, 1),\n",
      "  (43, 3),\n",
      "  (44, 1),\n",
      "  (45, 2),\n",
      "  (46, 2),\n",
      "  (47, 4),\n",
      "  (48, 1),\n",
      "  (49, 1),\n",
      "  (50, 2),\n",
      "  (51, 2)],\n",
      " [(29, 1),\n",
      "  (31, 1),\n",
      "  (52, 2),\n",
      "  (53, 1),\n",
      "  (54, 3),\n",
      "  (55, 2),\n",
      "  (56, 4),\n",
      "  (57, 1),\n",
      "  (58, 1),\n",
      "  (59, 1),\n",
      "  (60, 1),\n",
      "  (61, 1),\n",
      "  (62, 3),\n",
      "  (63, 2),\n",
      "  (64, 1),\n",
      "  (65, 3),\n",
      "  (66, 1),\n",
      "  (67, 1),\n",
      "  (68, 1),\n",
      "  (69, 6),\n",
      "  (70, 1),\n",
      "  (71, 1),\n",
      "  (72, 3),\n",
      "  (73, 1),\n",
      "  (74, 3),\n",
      "  (75, 1),\n",
      "  (76, 1),\n",
      "  (77, 2),\n",
      "  (78, 2),\n",
      "  (79, 2),\n",
      "  (80, 2),\n",
      "  (81, 2),\n",
      "  (82, 2),\n",
      "  (83, 3),\n",
      "  (84, 3),\n",
      "  (85, 1),\n",
      "  (86, 3),\n",
      "  (87, 1),\n",
      "  (88, 7),\n",
      "  (89, 1),\n",
      "  (90, 2),\n",
      "  (91, 1),\n",
      "  (92, 1),\n",
      "  (93, 1),\n",
      "  (94, 1),\n",
      "  (95, 10),\n",
      "  (96, 2),\n",
      "  (97, 2),\n",
      "  (98, 1),\n",
      "  (99, 2),\n",
      "  (100, 1),\n",
      "  (101, 1),\n",
      "  (102, 1),\n",
      "  (103, 9),\n",
      "  (104, 3),\n",
      "  (105, 1),\n",
      "  (106, 2),\n",
      "  (107, 7),\n",
      "  (108, 4),\n",
      "  (109, 1),\n",
      "  (110, 1),\n",
      "  (111, 20),\n",
      "  (112, 1),\n",
      "  (113, 4),\n",
      "  (114, 3),\n",
      "  (115, 1),\n",
      "  (116, 2),\n",
      "  (117, 2),\n",
      "  (118, 3),\n",
      "  (119, 1),\n",
      "  (120, 1),\n",
      "  (121, 1),\n",
      "  (122, 1),\n",
      "  (123, 1),\n",
      "  (124, 2),\n",
      "  (125, 1),\n",
      "  (126, 7),\n",
      "  (127, 1),\n",
      "  (128, 1),\n",
      "  (129, 2),\n",
      "  (130, 3),\n",
      "  (131, 4),\n",
      "  (132, 1),\n",
      "  (133, 7),\n",
      "  (134, 1),\n",
      "  (135, 1),\n",
      "  (136, 1),\n",
      "  (137, 1),\n",
      "  (138, 1),\n",
      "  (139, 2),\n",
      "  (140, 3),\n",
      "  (141, 1),\n",
      "  (142, 1),\n",
      "  (143, 3),\n",
      "  (144, 2),\n",
      "  (145, 3)]]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(corpus[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-10 11:37:12,285 : WARNING : scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "2020-06-10 11:37:12,307 : INFO : creating matrix with 50 documents and 1236 features\n"
     ]
    }
   ],
   "source": [
    "index = similarities.MatrixSimilarity(tf_idf[corpus])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 0.8795297\n",
      "30 0.053583592\n",
      "21 0.052521154\n",
      "35 0.046421032\n",
      "44 0.045003835\n",
      "9 0.044486463\n",
      "36 0.03858678\n",
      "26 0.03634316\n",
      "11 0.029298063\n",
      "3 0.028930094\n",
      "19 0.024728918\n",
      "46 0.022591515\n",
      "12 0.020241328\n",
      "16 0.019945148\n",
      "47 0.01970283\n",
      "48 0.019464679\n",
      "0 0.019399758\n",
      "4 0.018571319\n",
      "27 0.016681235\n",
      "7 0.015699157\n",
      "42 0.01491687\n",
      "5 0.014368719\n",
      "22 0.011496466\n",
      "2 0.011180134\n",
      "10 0.010040576\n",
      "49 0.0093471445\n",
      "23 0.009177843\n",
      "45 0.008241816\n",
      "20 0.00808267\n",
      "43 0.00787808\n",
      "14 0.0078047155\n",
      "41 0.0071079764\n",
      "8 0.0058987807\n",
      "1 0.0057456186\n",
      "31 0.0043042256\n",
      "37 0.0017856667\n",
      "24 0.001494641\n",
      "15 0.00093438564\n",
      "25 0.00068809255\n",
      "18 0.00067818176\n",
      "34 0.0006757497\n",
      "13 0.0005107222\n",
      "17 0.0004304413\n",
      "38 0.00041536556\n",
      "29 0.00024654975\n",
      "33 0.00018535976\n",
      "28 0.00017331389\n",
      "6 0.0\n",
      "32 0.0\n",
      "40 0.0\n"
     ]
    }
   ],
   "source": [
    "query_document = \"金融 虎讯 月 日 消息 今日 菏泽市 地方 金融 监督 管理局 发布 该市 失联 小额贷款 公司 公告 显示 菏泽市 牡丹区 恒顺 小额贷款 有限公司 情形 监管 系统 或市 县 两级 地方 金融 监管部门 市场 监管部门 预留 电话 取得联系\".split()\n",
    "query_bow = dictionary.doc2bow(query_document)\n",
    "sims = index[tf_idf[query_bow]]\n",
    "for document_number, score in sorted(enumerate(sims), key=lambda x: x[1], reverse=True):\n",
    "    print(document_number, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "金融 虎讯 月 日 消息 今日 菏泽市 地方 金融 监督 管理局 发布 该市 失联 小额贷款 公司 公告 显示 菏泽市 牡丹区 恒顺 小额贷款 有限公司 情形 监管 系统 或市 县 两级 地方 金融 监管部门 市场 监管部门 预留 电话 取得联系 办公 场所 已转 做 长期 未向 监管 系统 报送 相关 数据 情形 小额贷款 公司 长期 脱离 监管 经营 情况 较大 风险 隐患 现 公告 请 牡丹区 恒顺 小额贷款 有限公司 公告 三十日 主动 该局 提供 相关 资料 情况 逾期 未 主动 山东省 地方 金融 条例 相关 进一步 监管 措施 返回 搜狐 查看 责任编辑\n"
     ]
    }
   ],
   "source": [
    "print(documents[39])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-10 11:38:30,170 : INFO : loading projection weights from ../data/sgns.financial.char.bz2\n",
      "2020-06-10 11:38:36,546 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-10 11:38:37,096 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-10 11:38:37,132 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-10 11:38:37,673 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-10 11:38:37,694 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-10 11:38:37,854 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-10 11:38:37,933 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-10 11:38:37,966 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-10 11:38:38,044 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-10 11:38:38,128 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-10 11:38:38,191 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-10 11:38:38,214 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-10 11:38:38,352 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-10 11:38:38,462 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-10 11:38:38,622 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-10 11:38:38,693 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-10 11:38:38,823 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-10 11:38:38,900 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-10 11:38:38,982 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-10 11:38:39,177 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-10 11:38:39,322 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-10 11:38:39,697 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-10 11:38:39,771 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-10 11:38:39,993 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-10 11:38:40,528 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-10 11:38:40,673 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-10 11:38:41,345 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-10 11:38:41,923 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-10 11:38:42,697 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-10 11:38:42,874 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-10 11:38:44,588 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-10 11:38:44,768 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-10 11:38:44,855 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-10 11:38:45,468 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-10 11:38:45,984 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-10 11:38:46,286 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-10 11:38:46,491 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-10 11:38:46,602 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-10 11:38:47,632 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-10 11:38:48,255 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-10 11:38:51,346 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-10 11:38:51,544 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-10 11:38:52,493 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-10 11:38:52,729 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-10 11:38:52,747 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-10 11:38:53,095 : WARNING : duplicate word '--------------------------------------------------------------------------------------------------' in word2vec file, ignoring all but first\n",
      "2020-06-10 11:38:53,693 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-10 11:38:57,307 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-10 11:41:54,682 : INFO : duplicate words detected, shrinking matrix size from 467389 to 467341\n",
      "2020-06-10 11:41:54,683 : INFO : loaded (467341, 300) matrix from ../data/sgns.financial.char.bz2\n"
     ]
    }
   ],
   "source": [
    "#加载预训练金融预料w2v model\n",
    "from gensim.models import KeyedVectors\n",
    "word_vectors_char = KeyedVectors.load_word2vec_format('../data/sgns.financial.char.bz2') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-10 11:48:17,139 : INFO : Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "2020-06-10 11:48:17,140 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-06-10 11:48:17,141 : INFO : built Dictionary(294 unique tokens: ['CME', 'WTI', '一度', '下跌', '世界']...) from 2 documents (total 919 corpus positions)\n",
      "2020-06-10 11:48:18,103 : INFO : Removed 6 and 18 OOV words from document 1 and 2 (respectively).\n",
      "2020-06-10 11:48:18,103 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-06-10 11:48:18,104 : INFO : built Dictionary(267 unique tokens: ['CME', 'WTI', '一度', '下跌', '世界']...) from 2 documents (total 653 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance = 3.0320\n",
      "distance = 5.0842\n"
     ]
    }
   ],
   "source": [
    "distance = word_vectors_char.wmdistance(texts[3], texts[44]) #两篇原油宝的文章\n",
    "print('distance = %.4f' % distance)\n",
    "distance = word_vectors_char.wmdistance(texts[3], texts[45]) #一篇原油宝一篇疫情\n",
    "print('distance = %.4f' % distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pDL] *",
   "language": "python",
   "name": "conda-env-pDL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
