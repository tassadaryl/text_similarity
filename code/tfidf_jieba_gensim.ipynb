{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stopwords(stopwords_path):\n",
    "    with open(stopwords_path, 'r', encoding='utf-8') as f:\n",
    "        return [line.strip() for line in f]\n",
    "    \n",
    "def preprocess_data(corpus_path, stopwords):\n",
    "    corpus = []\n",
    "    with open(corpus_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            corpus.append(' '.join([word for word in jieba.lcut(line.strip()) if word not in stopwords]))\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "2020-06-14 15:55:18,916 : DEBUG : Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/m3/4yh806w92fdgcn0bk16ql7nw0000gn/T/jieba.cache\n",
      "2020-06-14 15:55:18,919 : DEBUG : Loading model from cache /var/folders/m3/4yh806w92fdgcn0bk16ql7nw0000gn/T/jieba.cache\n",
      "Loading model cost 0.640 seconds.\n",
      "2020-06-14 15:55:19,558 : DEBUG : Loading model cost 0.640 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "2020-06-14 15:55:19,559 : DEBUG : Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "n = 50\n",
    "stopwords_path = \"../data/stop_words.txt\"\n",
    "documents_path = \"../data/documents_first_\" + str(n) + \".txt\"\n",
    "stopwords = load_stopwords(stopwords_path)\n",
    "documents = preprocess_data(documents_path, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'董明珠 惊人 之语 炮轰 美的 怒斥 国产车 炮火 引向 大众 这下 本来 习惯于 看热闹 吃 瓜 群众 答应 踩 同行 骂 竞争对手 意见 敢动 老子 一亩 三分 不行 014 月 日 晚间 格力电器 董事长 兼 总裁 董明珠 接受 采访 时 赞同 黄奇帆 取消 住房 公积金 说 格力电器 3700 套 房子 员工 入住 未来 格力 员工 发一 套房 公积金 听听 话 饱汉不知饿汉饥 专程来 炫富 经济 下行 大众 钱包 吃紧 格力 本事 每名 员工 分 房子 本事 格力 分房 取消 全国 公积金 站长 想 问 一句 董 小姐 蠢 坏 确实 格力 优秀 年 营业 收入 1981.53 拥有 万名 员工 格力 万名 员工 发一 套房 站长 说 信 格力 真 员工 分房 中国 企业 众多 能发 房子 企业 凤毛麟角 特别 受 疫情 影响 众多 行业 暴击 企业 濒临 倒闭 活着 不错 每人 发 套房 格力 员工 公积金 取消 公积金 格力 确实 省下 一大笔钱 我国 亿人 贫富差距 取消 公积金 势必会 影响 人群 利益 也许 董明珠 换种 表述 特定 福利 企业 取消 缴纳 公积金 企业 节约 成本 用于 研发 创新 公积金 整体 实施 影响 我国 当初 建立 住房 公积金 制度 新加坡 学习 希望 强制性 缴纳 办法 集合 政府 企业 职工 三方 力量 解决 民众 购房 中国 最先 实行 公积金 政策 上海 全国 房地产 市场 发展 实行 公房 分配制度 家庭 人均 住房面积 七八 平方米 住 拥挤 居住 环境 急需 改善 公积金 强制 缴存 看似 个人收入 减少 长期 并非如此 民企 公积金 缴纳 比例 5% 12% 薪资 基数 缴纳 比例 6% 公司 6% 12% 公积金 存缴 数额 50006% 别看 元不多 长此以往 可不是 小数 缴纳 时间 公积金 买房 提取 大众 福利 特别 事业单位 公务员 群体 公积金 缴纳 金额 高 一般来说 公务员 月 公积金 扣除 比例 工资 12% 公积金 政策 国家 补贴 数额 公务员 一个月 公积金 工资 24% 民企 两倍 账面 工资 特别 高 公务员 群体 公积金 住 建部 人民银行 总行 统计 显示 年 全国 住房 公积金 缴存 总额 14549.46 上年 增长 12.29% 缴存 人数 机关 事业单位 工作人员 国企 职工 缴存 住房 公积金 比例 占 年 缴存 总额 60.16% 占 高 年 城镇 私 民 营 企业 城镇 企业 缴纳 住房 公积金 占 总额 19.5% 公积金 公务员 群体 至关重要 普通人 公积金 房价 高昂 公积金 居民 低息 房贷 唯一 渠道 全国 住房 公积金 年 年度报告 显示 年末 累计 发放 住房贷款 3334.82 万笔 85821.32 人员 置业 首 套房 贷款 年 公积金 利率 3.25% 二套 3.75% 远 商贷 首 套房 贷款 平均 利率 5.5% 假设 贷款 年 期限 公积金 贷款 商业贷款 节省 利息 约 笔 不小 资金 取消 公积金 将会 损害 大部分 利益 举个 例子 刘 缴纳 公积金 比例 12% 月 缴存 652 年 3.25% 利率 贷款 购买 一套 住房 每月 还款 可用 公积金 冲抵 979 每月 还款 多元 公积金 减轻 购房 压力 用处 很大 缴纳 住房 公积金 好处 少 缴纳 个人所得税 计算 个税 减去 住房 公积金 数额 不论是 单位 缴纳 缴纳 实惠 很大 公积金 提取 不断扩大 公积金 效率 提升 买房 装修 可用 租房 大病 提取 当今 疫情 部门 发出通知 新冠 肺炎 患者 提取 住房 公积金 用于 医疗 支出 买房 账户 里 公积金 退休 取出 生活 改善 说 公积金 用处 只能 租房 买房 公积金 表面 事 关乎 国家 大部 利益 开发商 公积金 制度 降低 买房 门槛 买房 人会 有利于 房企 发展 地方 政府 开发商 有钱 买 缴纳 土地 出让金 有利于 地方 财政 说 专家 说 取消 公积金 可取 稍微 动动脑 子 甄别 利弊 类 声音 就行 跑 偏 免责 声明 本文 腾讯 新闻 客户端 媒体 代表 腾讯网 观点 立场'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [[word for word in document.split()] for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "texts = [[token for token in text if frequency[token] > 2] for text in texts]\n",
    "# pprint(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-14 15:55:21,539 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-06-14 15:55:21,546 : INFO : built Dictionary(1236 unique tokens: ['15%', '一周', '世界', '主任', '人数']...) from 50 documents (total 10487 corpus positions)\n",
      "2020-06-14 15:55:21,547 : INFO : saving Dictionary object under ../data/first_50_doc.dict, separately None\n",
      "2020-06-14 15:55:21,548 : INFO : saved ../data/first_50_doc.dict\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(1236 unique tokens: ['15%', '一周', '世界', '主任', '人数']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.save('../data/first_' + str(n) + '_doc.dict')\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-14 15:55:21,559 : INFO : storing corpus in Matrix Market format to ../data/first_50_doc.mm\n",
      "2020-06-14 15:55:21,560 : INFO : saving sparse matrix to ../data/first_50_doc.mm\n",
      "2020-06-14 15:55:21,561 : INFO : PROGRESS: saving document #0\n",
      "2020-06-14 15:55:21,567 : INFO : saved 50x1236 matrix, density=6.827% (4219/61800)\n",
      "2020-06-14 15:55:21,568 : INFO : saving MmCorpus index to ../data/first_50_doc.mm.index\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize('../data/first_' + str(n) + '_doc.mm', corpus)\n",
    "# pprint(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-14 15:55:21,573 : INFO : collecting document frequencies\n",
      "2020-06-14 15:55:21,574 : INFO : PROGRESS: processing document #0\n",
      "2020-06-14 15:55:21,576 : INFO : calculating IDF weights for 50 documents and 1236 features (4219 matrix non-zeros)\n"
     ]
    }
   ],
   "source": [
    "from gensim import models, similarities\n",
    "tf_idf = models.TfidfModel(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1),\n",
      "  (1, 1),\n",
      "  (2, 1),\n",
      "  (3, 1),\n",
      "  (4, 1),\n",
      "  (5, 2),\n",
      "  (6, 1),\n",
      "  (7, 3),\n",
      "  (8, 4),\n",
      "  (9, 1),\n",
      "  (10, 2),\n",
      "  (11, 4),\n",
      "  (12, 1),\n",
      "  (13, 1),\n",
      "  (14, 1),\n",
      "  (15, 2),\n",
      "  (16, 1),\n",
      "  (17, 3),\n",
      "  (18, 1),\n",
      "  (19, 1),\n",
      "  (20, 1),\n",
      "  (21, 1),\n",
      "  (22, 1),\n",
      "  (23, 1),\n",
      "  (24, 2),\n",
      "  (25, 1),\n",
      "  (26, 1),\n",
      "  (27, 5),\n",
      "  (28, 1),\n",
      "  (29, 1),\n",
      "  (30, 1),\n",
      "  (31, 2),\n",
      "  (32, 2),\n",
      "  (33, 2),\n",
      "  (34, 1),\n",
      "  (35, 3),\n",
      "  (36, 2),\n",
      "  (37, 1),\n",
      "  (38, 5),\n",
      "  (39, 5),\n",
      "  (40, 1),\n",
      "  (41, 1),\n",
      "  (42, 1),\n",
      "  (43, 3),\n",
      "  (44, 1),\n",
      "  (45, 2),\n",
      "  (46, 2),\n",
      "  (47, 4),\n",
      "  (48, 1),\n",
      "  (49, 1),\n",
      "  (50, 2),\n",
      "  (51, 2)],\n",
      " [(29, 1),\n",
      "  (31, 1),\n",
      "  (52, 2),\n",
      "  (53, 1),\n",
      "  (54, 3),\n",
      "  (55, 2),\n",
      "  (56, 4),\n",
      "  (57, 1),\n",
      "  (58, 1),\n",
      "  (59, 1),\n",
      "  (60, 1),\n",
      "  (61, 1),\n",
      "  (62, 3),\n",
      "  (63, 2),\n",
      "  (64, 1),\n",
      "  (65, 3),\n",
      "  (66, 1),\n",
      "  (67, 1),\n",
      "  (68, 1),\n",
      "  (69, 6),\n",
      "  (70, 1),\n",
      "  (71, 1),\n",
      "  (72, 3),\n",
      "  (73, 1),\n",
      "  (74, 3),\n",
      "  (75, 1),\n",
      "  (76, 1),\n",
      "  (77, 2),\n",
      "  (78, 2),\n",
      "  (79, 2),\n",
      "  (80, 2),\n",
      "  (81, 2),\n",
      "  (82, 2),\n",
      "  (83, 3),\n",
      "  (84, 3),\n",
      "  (85, 1),\n",
      "  (86, 3),\n",
      "  (87, 1),\n",
      "  (88, 7),\n",
      "  (89, 1),\n",
      "  (90, 2),\n",
      "  (91, 1),\n",
      "  (92, 1),\n",
      "  (93, 1),\n",
      "  (94, 1),\n",
      "  (95, 10),\n",
      "  (96, 2),\n",
      "  (97, 2),\n",
      "  (98, 1),\n",
      "  (99, 2),\n",
      "  (100, 1),\n",
      "  (101, 1),\n",
      "  (102, 1),\n",
      "  (103, 9),\n",
      "  (104, 3),\n",
      "  (105, 1),\n",
      "  (106, 2),\n",
      "  (107, 7),\n",
      "  (108, 4),\n",
      "  (109, 1),\n",
      "  (110, 1),\n",
      "  (111, 20),\n",
      "  (112, 1),\n",
      "  (113, 4),\n",
      "  (114, 3),\n",
      "  (115, 1),\n",
      "  (116, 2),\n",
      "  (117, 2),\n",
      "  (118, 3),\n",
      "  (119, 1),\n",
      "  (120, 1),\n",
      "  (121, 1),\n",
      "  (122, 1),\n",
      "  (123, 1),\n",
      "  (124, 2),\n",
      "  (125, 1),\n",
      "  (126, 7),\n",
      "  (127, 1),\n",
      "  (128, 1),\n",
      "  (129, 2),\n",
      "  (130, 3),\n",
      "  (131, 4),\n",
      "  (132, 1),\n",
      "  (133, 7),\n",
      "  (134, 1),\n",
      "  (135, 1),\n",
      "  (136, 1),\n",
      "  (137, 1),\n",
      "  (138, 1),\n",
      "  (139, 2),\n",
      "  (140, 3),\n",
      "  (141, 1),\n",
      "  (142, 1),\n",
      "  (143, 3),\n",
      "  (144, 2),\n",
      "  (145, 3)]]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(corpus[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-14 15:55:21,592 : WARNING : scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "2020-06-14 15:55:21,613 : INFO : creating matrix with 50 documents and 1236 features\n"
     ]
    }
   ],
   "source": [
    "index = similarities.MatrixSimilarity(tf_idf[corpus])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 0.8795297\n",
      "30 0.053583592\n",
      "21 0.052521154\n",
      "35 0.046421032\n",
      "44 0.045003835\n",
      "9 0.044486463\n",
      "36 0.03858678\n",
      "26 0.03634316\n",
      "11 0.029298063\n",
      "3 0.028930094\n",
      "19 0.024728918\n",
      "46 0.022591515\n",
      "12 0.020241328\n",
      "16 0.019945148\n",
      "47 0.01970283\n",
      "48 0.019464679\n",
      "0 0.019399758\n",
      "4 0.018571319\n",
      "27 0.016681235\n",
      "7 0.015699157\n",
      "42 0.01491687\n",
      "5 0.014368719\n",
      "22 0.011496466\n",
      "2 0.011180134\n",
      "10 0.010040576\n",
      "49 0.0093471445\n",
      "23 0.009177843\n",
      "45 0.008241816\n",
      "20 0.00808267\n",
      "43 0.00787808\n",
      "14 0.0078047155\n",
      "41 0.0071079764\n",
      "8 0.0058987807\n",
      "1 0.0057456186\n",
      "31 0.0043042256\n",
      "37 0.0017856667\n",
      "24 0.001494641\n",
      "15 0.00093438564\n",
      "25 0.00068809255\n",
      "18 0.00067818176\n",
      "34 0.0006757497\n",
      "13 0.0005107222\n",
      "17 0.0004304413\n",
      "38 0.00041536556\n",
      "29 0.00024654975\n",
      "33 0.00018535976\n",
      "28 0.00017331389\n",
      "6 0.0\n",
      "32 0.0\n",
      "40 0.0\n"
     ]
    }
   ],
   "source": [
    "query_document = \"金融 虎讯 月 日 消息 今日 菏泽市 地方 金融 监督 管理局 发布 该市 失联 小额贷款 公司 公告 显示 菏泽市 牡丹区 恒顺 小额贷款 有限公司 情形 监管 系统 或市 县 两级 地方 金融 监管部门 市场 监管部门 预留 电话 取得联系\".split()\n",
    "query_bow = dictionary.doc2bow(query_document)\n",
    "sims = index[tf_idf[query_bow]]\n",
    "for document_number, score in sorted(enumerate(sims), key=lambda x: x[1], reverse=True):\n",
    "    print(document_number, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "金融 虎讯 月 日 消息 今日 菏泽市 地方 金融 监督 管理局 发布 该市 失联 小额贷款 公司 公告 显示 菏泽市 牡丹区 恒顺 小额贷款 有限公司 情形 监管 系统 或市 县 两级 地方 金融 监管部门 市场 监管部门 预留 电话 取得联系 办公 场所 已转 做 长期 未向 监管 系统 报送 相关 数据 情形 小额贷款 公司 长期 脱离 监管 经营 情况 较大 风险 隐患 现 公告 请 牡丹区 恒顺 小额贷款 有限公司 公告 三十日 主动 该局 提供 相关 资料 情况 逾期 未 主动 山东省 地方 金融 条例 相关 进一步 监管 措施 返回 搜狐 查看 责任编辑\n"
     ]
    }
   ],
   "source": [
    "print(documents[39])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-14 15:55:21,657 : INFO : loading projection weights from ../data/sgns.financial.char.bz2\n",
      "2020-06-14 15:55:27,548 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-14 15:55:28,113 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-14 15:55:28,145 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-14 15:55:28,690 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-14 15:55:28,713 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-14 15:55:28,882 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-14 15:55:28,948 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-14 15:55:28,975 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-14 15:55:29,037 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-14 15:55:29,115 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-14 15:55:29,179 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-14 15:55:29,205 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-14 15:55:29,356 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-14 15:55:29,477 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-14 15:55:29,630 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-14 15:55:29,691 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-14 15:55:29,799 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-14 15:55:29,868 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-14 15:55:29,957 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-14 15:55:30,129 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-14 15:55:30,247 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-14 15:55:30,676 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-14 15:55:30,760 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-14 15:55:30,976 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-14 15:55:31,559 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-14 15:55:31,694 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-14 15:55:32,458 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-14 15:55:33,100 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-14 15:55:33,878 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-14 15:55:34,089 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-14 15:55:35,742 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-14 15:55:35,899 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-14 15:55:35,983 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-14 15:55:36,628 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-14 15:55:37,093 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-14 15:55:37,414 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-14 15:55:37,620 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-14 15:55:37,710 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-14 15:55:38,671 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-14 15:55:39,351 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-14 15:55:42,624 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-14 15:55:42,832 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-14 15:55:43,847 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-14 15:55:44,091 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-14 15:55:44,110 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-14 15:55:44,503 : WARNING : duplicate word '--------------------------------------------------------------------------------------------------' in word2vec file, ignoring all but first\n",
      "2020-06-14 15:55:45,229 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-14 15:55:49,280 : WARNING : duplicate word '..................................................................................................' in word2vec file, ignoring all but first\n",
      "2020-06-14 15:58:28,927 : INFO : duplicate words detected, shrinking matrix size from 467389 to 467341\n",
      "2020-06-14 15:58:28,928 : INFO : loaded (467341, 300) matrix from ../data/sgns.financial.char.bz2\n"
     ]
    }
   ],
   "source": [
    "#加载预训练金融预料w2v model\n",
    "from gensim.models import KeyedVectors\n",
    "word_vectors_char = KeyedVectors.load_word2vec_format('../data/sgns.financial.char.bz2') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-14 15:58:28,934 : INFO : Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "2020-06-14 15:58:28,935 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-06-14 15:58:28,937 : INFO : built Dictionary(294 unique tokens: ['CME', 'WTI', '一度', '下跌', '世界']...) from 2 documents (total 919 corpus positions)\n",
      "2020-06-14 15:58:29,746 : INFO : Removed 6 and 18 OOV words from document 1 and 2 (respectively).\n",
      "2020-06-14 15:58:29,747 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-06-14 15:58:29,748 : INFO : built Dictionary(267 unique tokens: ['CME', 'WTI', '一度', '下跌', '世界']...) from 2 documents (total 653 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance = 3.0320\n",
      "distance = 5.0842\n"
     ]
    }
   ],
   "source": [
    "distance = word_vectors_char.wmdistance(texts[3], texts[44]) #两篇原油宝的文章\n",
    "print('distance = %.4f' % distance)\n",
    "distance = word_vectors_char.wmdistance(texts[3], texts[45]) #一篇原油宝一篇疫情\n",
    "print('distance = %.4f' % distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc2vec example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TaggedDocument(words=['新华社', '哥本哈根', '月', '日电', '记者', '林晶', '世界卫生组织', '欧洲', '区域', '办事处', '主任', '克卢格', '月', '日', '哥本哈根', '视频', '例会', '时', '呼吁', '区域', '各国', '特别', '社区', '传播', '控制', '国家', '应', '确保', '医疗卫生', '系统', '双轨制', '抗击', '新冠', '疫情', '保证', '常规', '医疗卫生', '服务', '运转', '克卢格', '说', '欧洲地区', '新冠', '疫情', '严峻', '一周', '累计', '确诊', '病例', '15%', '累计', '确诊', '病例', '达', '1408266', '例', '同期', '死亡', '病例', '18%', '累计', '死亡', '人数', '达', '129344', '欧洲地区', '累计', '确诊', '死亡', '病例', '占', '世界', '相关', '病例', '数', '46%', '63%', '克卢格', '呼吁', '欧洲各国', '政府', '卫生机构', '寻求', '办法', '控制', '新冠', '病毒', '社区', '传播', '前提', '快速', '恢复', '常规', '医疗卫生', '服务', '特别', '指出', '形势', '保证', '儿童', '接种', '麻疹', '常规', '疫苗', '重要性', '克卢格', '说', '新冠', '疫情', '短时期', '消失', '波', '第三', '波', '疫情', '认知', '动员', '社会', '理解', '协作', '双轨制', '医疗卫生', '系统', '保证', '应对', '新冠', '疫情', '反复', '时', '灵活性', '弹性'], tags=[0]), TaggedDocument(words=['昨晚', '美股', '率先', '突破', '站上', '日', '均线', '创新', '高', '东京', '日经指数', '涨幅', '超', '2%', '创新', '高', 'A股', '大板', '指', '高开高', '走', '上证', '一举', '拿下', '久攻不下', '2850', '点', '成交量', '暴增', '银行', '强', '银行', '板块', '昨天', '起到', '应有', '护盘', '作用', '轮', '证券', '板块', '启动', '大涨', '超', '2%', '上证指数', '支撑', '爆量', '上涨', '资金', '光速', '进场', '资金', '板块', '半导体', '板块', '板块', '指数', '暴涨', '7%', '创', '历史', '单日', '涨幅', '板块', '权重股', '行业龙头', '涨停', '如兆易', '创新', '通富', '微电', '长电', '科技', '沪', '硅', '产业', '板块', '效应', '板块', '效应', '吸引', '资金', '参与', '交易', '好事', '板块', '技术', '面', '短期', '均线', '均线', '粘合', '短期', '平均', '成本', '重叠', '爆量', '暴涨', '站', '上半年', '线', '近半年', '平均', '持仓', '成本', '获利', '人类', '趋利避害', '特性', '越', '赚钱', '越', '持股', '买入', '越', '亏损', '越', '卖出', '恐慌性', '吸引', '资金', '参与', '半导体', '指数', '无线耳机', '板块', '板块', '指数', '涨幅', '超', '5%', '佳禾', '智能', '天', '板', '市场', '龙头股', '华胜天', '成', '突破', '年', '高点', '涨停板', '亿', '资金', '抢筹', '封板', '突破', '趋势', '安洁', '科技', 'PE30', '倍', '估值', '不高', '流通', '市值', '亿', '年线', '启动', '早盘', '开盘', '分钟', '涨停', '速度', '之快', '前所未有', '资金', '心情', '急切', '可见一斑', '华胜天', '成', '科技股', '走势', '板块', '食品饮料', '农林牧渔', '人造肉', '医药', '板块', '究其原因', '逻辑', '是因为', '农林牧渔', '食品饮料', '人造肉', '医药', '消费', '板块', '偏', '防御', '指数', '一跌', '板块', '必涨', '这招', '屡试不爽', 'A股', '市场', '经验', '规律性', '质疑', '当作', '公理', '来记', '当作', '记住', '预计', '指数', '五一', '后先', '延续', '上涨', '态势', '科技股', '领涨', '交易日', '指数', '遇压', '调整', '指数', '调整', '防御性', '板块', '食品饮料', '农林牧渔', '人造肉', '医药', '复制', '前期', '行情', '下图', '农林牧渔', '板块', '指数', '上证指数', '叠加', '图', '上半', '农林牧渔', '板块', '指数', '上证指数', '走势', '农林牧渔', '上证指数', '叠加', '图', '透露', '观察', '市场', '情绪', '指标', '规律', '前天', '早盘', 'A股', '跌停', '数量', '上次', '跌停', '数量', '好巧', '巧合', '市场', '恐慌', '情绪', '聪明', '资金', '一看', '熟悉', '情况', '情绪', '低谷', '来临', '拐点', '到来', '久违', '百股', '涨停', '情绪', '时间', '周期', '情绪', '周期', '高点', '低点', '天', '时间', '低点', '高点', '天', '时间', '时间', '做', '防御性', '板块', '情绪', '低谷', '指标', '市场', '情绪', '退潮', '期时', '市场', '最先', '跌停', '数量', '增多', '连续', '跌停', '股', '数量', '增多', '恐慌性', '顶', '跌停', '数量', '只股', '跌停', '跌停', '数量', '减少', '涨停', '数量', '增多', '高潮', '指标', '充分利用', '市场', '情绪', '拐点', '情绪', '低点', '时', '布局', '情绪', '高潮', '规避', '转向', '防御性', '板块'], tags=[1])]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "train_corpus = []\n",
    "for i in range(n):\n",
    "    train_corpus.append(gensim.models.doc2vec.TaggedDocument(documents[i].split(), [i]))\n",
    "print(train_corpus[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-14 16:20:31,932 : INFO : collecting all words and their counts\n",
      "2020-06-14 16:20:31,933 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2020-06-14 16:20:31,937 : INFO : collected 5162 word types and 50 unique tags from a corpus of 50 examples and 15183 words\n",
      "2020-06-14 16:20:31,938 : INFO : Loading a fresh vocabulary\n",
      "2020-06-14 16:20:31,945 : INFO : effective_min_count=2 retains 2006 unique words (38% of original 5162, drops 3156)\n",
      "2020-06-14 16:20:31,945 : INFO : effective_min_count=2 leaves 12027 word corpus (79% of original 15183, drops 3156)\n",
      "2020-06-14 16:20:31,954 : INFO : deleting the raw counts dictionary of 5162 items\n",
      "2020-06-14 16:20:31,955 : INFO : sample=0.001 downsamples 41 most-common words\n",
      "2020-06-14 16:20:31,956 : INFO : downsampling leaves estimated 11177 word corpus (92.9% of prior 12027)\n",
      "2020-06-14 16:20:31,961 : INFO : estimated required memory for 2006 words and 20 dimensions: 1327960 bytes\n",
      "2020-06-14 16:20:31,962 : INFO : resetting layer weights\n",
      "2020-06-14 16:20:32,378 : INFO : training model with 3 workers on 2006 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-06-14 16:20:32,385 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-14 16:20:32,392 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-14 16:20:32,399 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-14 16:20:32,399 : INFO : EPOCH - 1 : training on 15183 raw words (11222 effective words) took 0.0s, 756149 effective words/s\n",
      "2020-06-14 16:20:32,401 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-14 16:20:32,408 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-14 16:20:32,415 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-14 16:20:32,415 : INFO : EPOCH - 2 : training on 15183 raw words (11242 effective words) took 0.0s, 779082 effective words/s\n",
      "2020-06-14 16:20:32,417 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-14 16:20:32,425 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-14 16:20:32,431 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-14 16:20:32,432 : INFO : EPOCH - 3 : training on 15183 raw words (11244 effective words) took 0.0s, 767434 effective words/s\n",
      "2020-06-14 16:20:32,434 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-14 16:20:32,441 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-14 16:20:32,447 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-14 16:20:32,448 : INFO : EPOCH - 4 : training on 15183 raw words (11240 effective words) took 0.0s, 805387 effective words/s\n",
      "2020-06-14 16:20:32,450 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-14 16:20:32,456 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-14 16:20:32,463 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-14 16:20:32,463 : INFO : EPOCH - 5 : training on 15183 raw words (11253 effective words) took 0.0s, 805981 effective words/s\n",
      "2020-06-14 16:20:32,466 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-14 16:20:32,472 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-14 16:20:32,482 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-14 16:20:32,482 : INFO : EPOCH - 6 : training on 15183 raw words (11252 effective words) took 0.0s, 656413 effective words/s\n",
      "2020-06-14 16:20:32,486 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-14 16:20:32,493 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-14 16:20:32,499 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-14 16:20:32,500 : INFO : EPOCH - 7 : training on 15183 raw words (11218 effective words) took 0.0s, 763664 effective words/s\n",
      "2020-06-14 16:20:32,502 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-14 16:20:32,509 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-14 16:20:32,515 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-14 16:20:32,516 : INFO : EPOCH - 8 : training on 15183 raw words (11225 effective words) took 0.0s, 756905 effective words/s\n",
      "2020-06-14 16:20:32,518 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-14 16:20:32,525 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-14 16:20:32,531 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-14 16:20:32,531 : INFO : EPOCH - 9 : training on 15183 raw words (11226 effective words) took 0.0s, 850757 effective words/s\n",
      "2020-06-14 16:20:32,534 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-14 16:20:32,540 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-14 16:20:32,547 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-14 16:20:32,547 : INFO : EPOCH - 10 : training on 15183 raw words (11231 effective words) took 0.0s, 815636 effective words/s\n",
      "2020-06-14 16:20:32,550 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-14 16:20:32,556 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-14 16:20:32,563 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-14 16:20:32,564 : INFO : EPOCH - 11 : training on 15183 raw words (11206 effective words) took 0.0s, 786735 effective words/s\n",
      "2020-06-14 16:20:32,566 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-14 16:20:32,574 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-14 16:20:32,581 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-14 16:20:32,582 : INFO : EPOCH - 12 : training on 15183 raw words (11256 effective words) took 0.0s, 704968 effective words/s\n",
      "2020-06-14 16:20:32,585 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-14 16:20:32,592 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-14 16:20:32,600 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-14 16:20:32,601 : INFO : EPOCH - 13 : training on 15183 raw words (11245 effective words) took 0.0s, 676966 effective words/s\n",
      "2020-06-14 16:20:32,604 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-14 16:20:32,610 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-14 16:20:32,618 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-14 16:20:32,618 : INFO : EPOCH - 14 : training on 15183 raw words (11238 effective words) took 0.0s, 756703 effective words/s\n",
      "2020-06-14 16:20:32,620 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-14 16:20:32,627 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-14 16:20:32,635 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-14 16:20:32,635 : INFO : EPOCH - 15 : training on 15183 raw words (11228 effective words) took 0.0s, 741360 effective words/s\n",
      "2020-06-14 16:20:32,638 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-14 16:20:32,645 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-14 16:20:32,651 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-14 16:20:32,652 : INFO : EPOCH - 16 : training on 15183 raw words (11210 effective words) took 0.0s, 784468 effective words/s\n",
      "2020-06-14 16:20:32,654 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-14 16:20:32,661 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-14 16:20:32,668 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-14 16:20:32,669 : INFO : EPOCH - 17 : training on 15183 raw words (11221 effective words) took 0.0s, 741530 effective words/s\n",
      "2020-06-14 16:20:32,672 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-14 16:20:32,678 : INFO : worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-14 16:20:32,685 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-14 16:20:32,685 : INFO : EPOCH - 18 : training on 15183 raw words (11217 effective words) took 0.0s, 791559 effective words/s\n",
      "2020-06-14 16:20:32,687 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-14 16:20:32,694 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-14 16:20:32,701 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-14 16:20:32,701 : INFO : EPOCH - 19 : training on 15183 raw words (11234 effective words) took 0.0s, 765080 effective words/s\n",
      "2020-06-14 16:20:32,704 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-14 16:20:32,710 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-14 16:20:32,716 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-14 16:20:32,717 : INFO : EPOCH - 20 : training on 15183 raw words (11219 effective words) took 0.0s, 836516 effective words/s\n",
      "2020-06-14 16:20:32,719 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-14 16:20:32,727 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-14 16:20:32,735 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-14 16:20:32,735 : INFO : EPOCH - 21 : training on 15183 raw words (11228 effective words) took 0.0s, 680964 effective words/s\n",
      "2020-06-14 16:20:32,739 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-14 16:20:32,747 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-14 16:20:32,754 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-14 16:20:32,754 : INFO : EPOCH - 22 : training on 15183 raw words (11193 effective words) took 0.0s, 703642 effective words/s\n",
      "2020-06-14 16:20:32,757 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-14 16:20:32,764 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-14 16:20:32,770 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-14 16:20:32,770 : INFO : EPOCH - 23 : training on 15183 raw words (11252 effective words) took 0.0s, 832036 effective words/s\n",
      "2020-06-14 16:20:32,772 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-14 16:20:32,779 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-14 16:20:32,786 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-14 16:20:32,786 : INFO : EPOCH - 24 : training on 15183 raw words (11227 effective words) took 0.0s, 786640 effective words/s\n",
      "2020-06-14 16:20:32,788 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-14 16:20:32,795 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-14 16:20:32,801 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-14 16:20:32,802 : INFO : EPOCH - 25 : training on 15183 raw words (11220 effective words) took 0.0s, 819969 effective words/s\n",
      "2020-06-14 16:20:32,805 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-14 16:20:32,812 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-14 16:20:32,819 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-14 16:20:32,819 : INFO : EPOCH - 26 : training on 15183 raw words (11257 effective words) took 0.0s, 725033 effective words/s\n",
      "2020-06-14 16:20:32,822 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-14 16:20:32,830 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-14 16:20:32,835 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-14 16:20:32,836 : INFO : EPOCH - 27 : training on 15183 raw words (11255 effective words) took 0.0s, 787657 effective words/s\n",
      "2020-06-14 16:20:32,838 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-14 16:20:32,845 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-14 16:20:32,850 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-14 16:20:32,851 : INFO : EPOCH - 28 : training on 15183 raw words (11223 effective words) took 0.0s, 821392 effective words/s\n",
      "2020-06-14 16:20:32,853 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-14 16:20:32,860 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-14 16:20:32,866 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-14 16:20:32,866 : INFO : EPOCH - 29 : training on 15183 raw words (11214 effective words) took 0.0s, 806072 effective words/s\n",
      "2020-06-14 16:20:32,868 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-14 16:20:32,875 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-14 16:20:32,882 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-14 16:20:32,882 : INFO : EPOCH - 30 : training on 15183 raw words (11213 effective words) took 0.0s, 771622 effective words/s\n",
      "2020-06-14 16:20:32,884 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-14 16:20:32,893 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-14 16:20:32,900 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-14 16:20:32,900 : INFO : EPOCH - 31 : training on 15183 raw words (11240 effective words) took 0.0s, 694437 effective words/s\n",
      "2020-06-14 16:20:32,904 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-14 16:20:32,911 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-14 16:20:32,917 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-14 16:20:32,918 : INFO : EPOCH - 32 : training on 15183 raw words (11224 effective words) took 0.0s, 773529 effective words/s\n",
      "2020-06-14 16:20:32,920 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-14 16:20:32,927 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-14 16:20:32,933 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-14 16:20:32,933 : INFO : EPOCH - 33 : training on 15183 raw words (11241 effective words) took 0.0s, 825381 effective words/s\n",
      "2020-06-14 16:20:32,936 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-14 16:20:32,942 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-14 16:20:32,948 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-14 16:20:32,948 : INFO : EPOCH - 34 : training on 15183 raw words (11229 effective words) took 0.0s, 830324 effective words/s\n",
      "2020-06-14 16:20:32,950 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-14 16:20:32,957 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-14 16:20:32,963 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-14 16:20:32,964 : INFO : EPOCH - 35 : training on 15183 raw words (11228 effective words) took 0.0s, 828428 effective words/s\n",
      "2020-06-14 16:20:32,966 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-14 16:20:32,972 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-14 16:20:32,980 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-14 16:20:32,981 : INFO : EPOCH - 36 : training on 15183 raw words (11218 effective words) took 0.0s, 726151 effective words/s\n",
      "2020-06-14 16:20:32,983 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-14 16:20:32,989 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-14 16:20:32,996 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-14 16:20:32,996 : INFO : EPOCH - 37 : training on 15183 raw words (11223 effective words) took 0.0s, 819186 effective words/s\n",
      "2020-06-14 16:20:32,998 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-14 16:20:33,005 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-14 16:20:33,010 : INFO : worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-14 16:20:33,011 : INFO : EPOCH - 38 : training on 15183 raw words (11261 effective words) took 0.0s, 839159 effective words/s\n",
      "2020-06-14 16:20:33,013 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-14 16:20:33,020 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-14 16:20:33,026 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-14 16:20:33,027 : INFO : EPOCH - 39 : training on 15183 raw words (11219 effective words) took 0.0s, 813547 effective words/s\n",
      "2020-06-14 16:20:33,029 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-14 16:20:33,035 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-14 16:20:33,041 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-14 16:20:33,041 : INFO : EPOCH - 40 : training on 15183 raw words (11237 effective words) took 0.0s, 880024 effective words/s\n",
      "2020-06-14 16:20:33,042 : INFO : training on a 607320 raw words (449229 effective words) took 0.7s, 677296 effective words/s\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=20, min_count=2, epochs=40)\n",
    "model.build_vocab(train_corpus)\n",
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.2199221   0.23610279 -0.04548362  0.02160881  0.3588981  -0.1897905\n",
      " -0.01976469 -0.01329631 -0.17440309 -0.01348257  0.03435411 -0.15655991\n",
      "  0.2260536  -0.17815651  0.01567412 -0.12317552 -0.03368188  0.13903558\n",
      "  0.2561948  -0.09461357]\n"
     ]
    }
   ],
   "source": [
    "vector = model.infer_vector(['金融','行业','原油宝','期货'])\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-14 16:24:38,932 : INFO : precomputing L2-norms of doc weight vectors\n"
     ]
    }
   ],
   "source": [
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(train_corpus)):\n",
    "    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "\n",
    "    second_ranks.append(sims[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 46, 1: 4})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "counter = collections.Counter(ranks)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document (49): «雷帝 网雷 建平 月 日 报道 氪 今日 SEC 公布 F 文件 文件 显示 年 月 日 氪 拥有 937 358 520 股 普通股 包括 841 275 820 股 类 普通股 082 股 B 类 普通股 年 月 日 氪 CEO 冯大刚 持有 16.3% 股权 75.5% 投票权 氪 创始人 刘成城 持有 6.3% 股权 32.2% 投票权 冯大刚 刘成 城是 冯大刚 投票权 包含 刘成城 投票权 蚂蚁 金服 持股 16.2% 拥有 4.7% 投票权 TembusuLimited 持股 10.8% 拥有 3.1% ChinaProsperityCapitalAlphaLimited 持股 7.6% 拥有 2.2% 投票权 BeijingJiuheYunqiInvestmentCenterL P 持股 7% 拥有 2% 投票权 M36InvestmentLimited 持股 6.7% 拥有 1.9% 投票权 文件 显示 财年 氪 收入 总额 6.56 约 9420 万美元 财年 2.99 相比 增长 119.2% 财年 氪 线 广告 类 收入 2.83 约 4070 万美元 上年 同期 增长 63.1% 企业 增值 服务 类 收入 3.20 约 4590 万美元 上年 同期 增长 218.7% 订阅 服务 类 收入 5270 约 美元 760 上年 同期 增长 110.2% 财年 氪 线 广告 类 收入 占 43.2% 企业 增值 服务 类 收入 占 48.8% 订阅 服务 类 收入 占 8.0% 财年 氪 服务 线 广告 客户 数较 财年 增长 58.1% 企业 增值 服务 客户 数 增长 65.8% 财年 氪 毛利 2.75 约 美元 3950 上年 同期 1.59 相比 增长 73.4% 财年 氪 调整 非 美国通用 会计准则 Non GAAP 净利润 6530 约 940 万美元 相比 上年 同期 4560 增长 43.0% 氪 年 月 美国 上市 市值 1.33 亿美元 上市 时跌 雷帝 触网 资深 媒体 人雷 建平 创办 转载 请 写明 来源 返回 搜狐 查看 责任编辑»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d20,n5,w5,mc2,s0.001,t3):\n",
      "\n",
      "MOST (49, 0.9094815254211426): «雷帝 网雷 建平 月 日 报道 氪 今日 SEC 公布 F 文件 文件 显示 年 月 日 氪 拥有 937 358 520 股 普通股 包括 841 275 820 股 类 普通股 082 股 B 类 普通股 年 月 日 氪 CEO 冯大刚 持有 16.3% 股权 75.5% 投票权 氪 创始人 刘成城 持有 6.3% 股权 32.2% 投票权 冯大刚 刘成 城是 冯大刚 投票权 包含 刘成城 投票权 蚂蚁 金服 持股 16.2% 拥有 4.7% 投票权 TembusuLimited 持股 10.8% 拥有 3.1% ChinaProsperityCapitalAlphaLimited 持股 7.6% 拥有 2.2% 投票权 BeijingJiuheYunqiInvestmentCenterL P 持股 7% 拥有 2% 投票权 M36InvestmentLimited 持股 6.7% 拥有 1.9% 投票权 文件 显示 财年 氪 收入 总额 6.56 约 9420 万美元 财年 2.99 相比 增长 119.2% 财年 氪 线 广告 类 收入 2.83 约 4070 万美元 上年 同期 增长 63.1% 企业 增值 服务 类 收入 3.20 约 4590 万美元 上年 同期 增长 218.7% 订阅 服务 类 收入 5270 约 美元 760 上年 同期 增长 110.2% 财年 氪 线 广告 类 收入 占 43.2% 企业 增值 服务 类 收入 占 48.8% 订阅 服务 类 收入 占 8.0% 财年 氪 服务 线 广告 客户 数较 财年 增长 58.1% 企业 增值 服务 客户 数 增长 65.8% 财年 氪 毛利 2.75 约 美元 3950 上年 同期 1.59 相比 增长 73.4% 财年 氪 调整 非 美国通用 会计准则 Non GAAP 净利润 6530 约 940 万美元 相比 上年 同期 4560 增长 43.0% 氪 年 月 美国 上市 市值 1.33 亿美元 上市 时跌 雷帝 触网 资深 媒体 人雷 建平 创办 转载 请 写明 来源 返回 搜狐 查看 责任编辑»\n",
      "\n",
      "SECOND-MOST (27, 0.6663030385971069): «新 京报 快讯 记者 冯琪 月 日 A股 上市 早教 公司 美 吉姆 教育 发布 年 第一季度 报告 营收 0.57 上年 同期 下降 53.52% 归属于 上市公司 股东 净利润 471 上年 同期 下降 127.90% 财报 显示 年末 美 吉姆 全国 签约 早教 中心 共计 524 家 业绩 亏损 原因 美 吉姆 财报 解释 称 新冠 肺炎 疫情 暴发 导致 全国 美 吉姆 中心 营业 收入 下降 美 吉姆 推出 在线 业务 财报 显示 年 月 日 美 吉姆 在线 上线 小时 注册 用户 突破 年 月 日 总 注册 用户 约 美 吉姆 既有 会员 在线 课程 包括 英文 儿歌 课 亲子 互动 课 手偶 故事 课 美 吉姆 称 此举 有助于 线下 中心 保障 现有 客户 黏性 增加 潜在 客户 数量 同日 美 吉姆 发布 年 年度报告 年 公司 营收 6.3 去年同期 137.35% 归属于 上市公司 普通股 股东 净利润 1.2 去年同期 279.40% 美 吉姆 教育 服务 主营业务 年 月 上市公司 现名 大连 美 吉姆 教育 科技股份 有限公司 转型 教育 企业 旗下 早教 品牌 美 吉姆 拥有 国内 低龄 留学 语培 服务 品牌 楷德 教育 新 京报 记者 冯琪 校对 薛京宁»\n",
      "\n",
      "MEDIAN (28, 0.41456782817840576): «新 京报 讯 记者 戚望 月 日 记者 长安 大学 获悉 该校 首个 中外合作 办学 机构 长安 大学 长安 都柏林 国际 交通 学院 正式 获批 成立 采用 双学位 培养 模式 国际化 教学 体系 长安 大学 称 学院 开设 道路 桥梁 渡河 工程 车辆 工程 交通运输 本科专业 计划 年 招生 专业 招收 120 学院 建设 交通运输 工程 领域 特色 鲜明 国际化 学院 目标 培养 交通运输 工程 领域 广阔 国际 视野 工程 解决 能力 全球 胜任 力 复合型 创新型 新 工科 交通 国际化 高端 技术 管理 人才 学院 采用 双学位 培养 模式 国际化 教学 体系 学生 两校 注册 拥有 双 学籍 学生 国内 学习 毕业 时 长安 大学本科 毕业证书 学士学位 证书 爱尔兰 都柏林 大学 学士学位 证书 长安 都柏林 国际 交通 学院 长安 大学 首个 中外合作 办学 机构 外方 合作 高校 爱尔兰 都柏林 大学 创立 1854 年 2020QS 世界 大学排名 185 位 工程技术 学科 2019QS 世界 学科 排名 123 位 新 京报 记者 戚望 校对 薛京宁»\n",
      "\n",
      "LEAST (47, 0.010571189224720001): «华夏 时报 chinatimes net 记者 建平 北京 报道 利润 同比 下降 宁德 时代 财报 新冠 病毒 疫情 影响 市场 影响 新能源 汽车 装机量 大幅 下降 导致 公司 一季度 动力电池 销售收入 下滑 归属于 上市公司 股东 净利润 上年 同比 下降 疫情 拖累 业绩 下滑 宁德 时代 季报 利润 同比 下滑 意料之中 事 中国汽车工业协会 发布 数据 显示 年 一季度 国内 新能源 汽车 产销量 10.5 万辆 11.4 万辆 同比 下降 60.2% 56.4% 电动汽车 产销量 7.7 万辆 8.5 万辆 同比 下降 61.8% 58.6% 年 新能源 汽车 市场 增速 放缓 动力电池 装机量 同比 增加 10.2% 相比 年近 增幅 大幅 缩窄 年 新冠 疫情 爆发 市场 再受 打击 年 一季度 动力电池 整体 装机量 5.68 GWh 同比 下降 46.2% 包括 宁德 时代 比亚迪 在内 多家 动力电池 企业 一季度 装机量 同比 程度 下滑 装机量 排名 动力电池 企业 LG 化学 塔 菲尔 欣旺达 同比 增长 同比 下跌 企业 平均 跌幅 55.6% 占 超过 营收 板块 动力电池 板块 下挫 很大 程度 影响 宁德 时代 一季度 业绩 表现 年 中国 新能源 汽车 市场 十年 首次 下滑 宁德 时代 无太大 影响 年 宁德 时代 动力电池 出货量 32.5 GWh 同比 上涨 38.89% 同期 全球 动力电池 出货量 增速 16.6% 一半 宁德 时代 年报 显示 年 营收 457.9 同比 增长 54.63% 近三年 营收 增长 新高 点 归属于 上市公司 股东 净利润 45.6 同比 增长 34.64% 归母 净利润 45.6 同比 增长 34.64% 宁德 时代 业务 板块 动力电池 系统 锂电池 材料 储能 系统对 营收 贡献率 84.27% 9.4% 1.33% 主力 动力电池 系统 营收 增长率 高达 57.38% 工信部 公布 年 新能源 车型 目录 中有 4600 余款 搭载 宁德 时代 电池 余款 占 比约 41.5% SNEResearch 发布 动力电池 出货量 数据 宁德 时代 连续 全球 动力电池 出货量 排名 第一位 松下 LG 化学 多方 竞争 格局 看似 大好 形势 之下 却是 暗涛 汹涌 竞争 格局 多方 压力 迎面而来 外资 电池 涌入 中国 市场 主机厂 选择 年 月 LG 化学 吉利 汽车 成立 合资 公司 动力电池 相关 研发 制造 销售 售后服务 业务 预计 年 吉利 电动汽车 供货 同年 月 韩国 SKInnovation 常州 建立 动力电池 厂 预计 年产 7.5 GWh 广汽传祺 动力电池 主 供应商 改为 中航 锂电 长安汽车 比亚迪 成立 动力电池 生产 销售 合资 公司 年 月 比亚迪 注册 成立 弗迪 公司 公司 电池 领域 具备 自主 研发 设计 生产能力 产品 覆盖 消费类 3C 电池 动力电池 储能 电池 梯次 利用 领域 月 日 比亚迪 对外 发布 新一代 磷酸 铁 锂电池 刀片 电池 刀片 电池 单位 体积 能量 密度 传统 铁 锂电池 提升 兼具 循环 寿命 续航 里程 优势 宁德 时代 技术 优势 很大 削弱 作用 特斯拉 沃尔沃 车企 独立 建立 电池 生产线 大型 整车 企业 甘心 核心 动力 模块 拱手让人 年 特斯拉 交付 36.8 万台 汽车 全球 电动汽车 销量 排行榜 遥遥领先 国产 Model3 搭载 松下 LG 化学 电池 未来 关系 改变 势必 宁德 时代 市 占率 不利 影响 年 月 宁德 时代 如愿 特斯拉 供应链 宁德 时代 公告 称 年 月 特斯拉 提供 锂离子 动力电池 供货 协议 持续 年 月 日 细节 并未 公开 加码 规划 海外 项目 电池 产业 竞争 格局 宁德 时代 清醒认识 宁德 时代 新能源 汽车 市场 快速 发展 市场竞争 日趋激烈 公司 未来 业务 发展 面临 市场竞争 加剧 风险 上游 原材料 钴 宁德 时代 技术 路线 三元 锂电池 关键 材料 资源 稀缺 导致 价格 上涨 未来 市场竞争 加剧 国家 政策 调整 因素 公司 产品 售价 原材料 采购 价格 发生 不利 变化 公司 毛利率 下降 风险 宁德 时代 财报 表达 原材料 上涨 导致 利润 下跌 担忧 未来 市场 预期 宁德 时代 财报 国内外 宏观经济 不确定性 年 新型 冠状病毒 肺炎 疫情 全球 经济 新能源 汽车 生产 销售 较大 影响 未来 疫情 全球 持续 扩散 宏观经济 消费者 消费 意愿 下滑 影响 新能源 动力电池 行业 发展 公司 经营 业绩 财务状况 不利 影响 应对 激烈 竞争 格局 宁德 时代 目光 投向 国际 市场 月 日 晚间 宁德 时代 发布公告 称 拟将 境外 发行 债券 额度 增加 超过 亿美元 等值 币种 年 月 宁德 时代 位于 德国 图林根 州 首个 海外 工厂 正式 破土 开工 计划 宁德 时代 欧洲 工厂 开工 面积 公顷 生产线 包括 电芯及 模组 产品 预计 年 14GWh 电池 产能 宁德 时代 戴姆勒 卡车 巴士 VWCO 国际联盟 Consortium 宝马 合作 项目 定点 供应商 主动出击 宁德 时代 稳 坐在 市占率 第一 位置 难以 给出 答案 编辑 孙斌 主编 赵云 返回 搜狐 查看 责任编辑»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Document (24): «月 日 深圳市 发改委 发布 应对 新冠 肺炎 疫情 影响 深圳市 新能源 汽车 推广应用 若干 措施 征求意见 稿 简称 征求意见 稿 征求意见 稿 提出 放宽 新能源 小汽车 增量 指标 申请 条件 增购 新能源 小汽车 车型 新购 新能源 小汽车 给予 综合 财政补贴 置换 更新 新能源 小汽车 实施 财政补贴 加大 新能源 汽车 停车 优惠 力度 措施 征求意见 稿 提出 取消 非深 户籍 人员 连续 月 本市 缴纳 补缴 医疗保险 华侨 港澳台地区 居民 年内 本市 累计 居住 月 外国人 连续 年且 本市 累计 居住 月 条件 征求意见 稿 提出 新购 新能源 小汽车 消费者 给予 综合 财政补贴 新购 电动 高级 型 经济型 乘用车 补贴 车 新购 插电式 混合 动力 高级 型 乘用车 补贴 车 应对 新冠 肺炎 疫情 影响 深圳市 新能源 汽车 推广应用 若干 措施 征求意见 稿 贯彻落实 中央 省 市 统筹 推进 疫情 防控 经济社会 发展 工作 部署 消费 扩容 提质 加快 强大 国内 市场 实施 意见 发改 就业 293 号 文件精神 进一步 推动 深圳市 新能源 汽车 推广应用 新能源 汽车 市场 消费 特 制定 政策措施 放宽 新能源 小汽车 增量 指标 申请 条件 持 深圳市 居住证 非深 户籍 人员 持 身份证明 本市 公安机关 办理 境外 人员 临时 住宿 登记 华侨 港澳台地区 居民 本市 办理 签证 居留 许可 外国人 申请 深圳市 混合 动力 小汽车 增量 指标 或纯 电动 小汽车 增量 指标 取消 非深 户籍 人员 连续 月 本市 缴纳 补缴 医疗保险 华侨 港澳台地区 居民 年内 本市 累计 居住 月 外国人 连续 年且 本市 累计 居住 月 条件 增购 新能源 小汽车 车型 名下 辆 深圳市 登记 小汽车 或仅 持有 深圳 小汽车 增量 指标 或仅 持有 深圳 小汽车 更新 指标 申请 资格 消费者 增购 新能源 小汽车 车型 由纯 电动 小汽车 至纯 电动 小汽车 插 电式 混合 动力 小汽车 新购 新能源 小汽车 给予 综合 财政补贴 新购 新能源 小汽车 消费者 给予 综合 财政补贴 新购 电动 高级 型 经济型 乘用车 补贴 车 新购 插电式 混合 动力 高级 型 乘用车 补贴 车 新购 新能源 小汽车 须 深圳市 注册 登记 汽车 销售 企业 购买 新车 机动车 销售 发票 销货 单位 须 深圳市 注册 登记 企业 置换 更新 新能源 小汽车 实施 财政补贴 原有 深圳市 牌照 燃油 小汽车 新能源 小汽车 旧车 置换 更新 新能源 小汽车 消费者 给予 财政补贴 更新 置换 电动 高级 型 插 电式 混合 动力 高级 型 乘用车 补贴 车 更新 置换 电动 经济型 乘用车 补贴 车 置换 更新 新能源 小汽车 须 深圳市 注册 登记 汽车 销售 企业 购买 新车 机动车 销售 发票 销货 单位 须 深圳市 注册 登记 企业 加大 新能源 汽车 停车 优惠 力度 全市 路内 停车位 给予 新能源 汽车 当日 免 首次 首 小时 临时 停车费 基础 增加 超过 小时 免 临时 停车费 优惠 政策措施 自本 文件 印发 实施 年 月 日止 内容 第一 财经 原创 著作权 第一 财经 未经 第一 财经 书面 授权 方式 包括 转载 摘编 复制 建立 镜像 第一 财经 保留 追究 侵权者 法律责任 权利 如需 授权 请 第一 财经 版权 部 021 22002972 021 22002335 banquan yicai»\n",
      "\n",
      "Similar Document (34, 0.6169401407241821): «月 日 陕西省 岐山县 一台 农用 机械 麦田 里 喷洒 农药 眼下 小麦 条锈病 防治 关键时期 陕西 农业部门 组织 专家 技术人员 田间 地头 防治 技术 指导 依托 专业化 防治 队伍 发动群众 防控 全省 累计 防控 面积 2600 万多亩 次 条锈病 防控 1368.6 万亩 次 新华社 记者 红刚 摄»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pick a random document from the corpus and infer a vector from the model\n",
    "import random\n",
    "doc_id = random.randint(0, len(train_corpus) - 1)\n",
    "\n",
    "# Compare and print the second-most-similar document\n",
    "print('Train Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "sim_id = second_ranks[doc_id]\n",
    "print('Similar Document {}: «{}»\\n'.format(sim_id, ' '.join(train_corpus[sim_id[0]].words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pDL] *",
   "language": "python",
   "name": "conda-env-pDL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
